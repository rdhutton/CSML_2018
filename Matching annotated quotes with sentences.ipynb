{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03-JUL-18\n",
    "\n",
    "Adaptation of \"LIME - basic LR\"\n",
    "\n",
    "Trains on the hyperpartisan data\n",
    "\n",
    "Tests on annotated data from Briefr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/rick/factmata/factnlp-experimental/lime')\n",
    "\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/rick/factmata/factmata-quality-engine/factnlp')\n",
    "sys.path.append('/Users/rick/factmata/utils')\n",
    "sys.path.append('/Users/rick/factmata/fastText')\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\": \"INFO\", \"message\": \"'pattern' package not found; tag filters are not available for English\", \"log_timestamp\": \"2018-08-25 18:32:55.487970\", \"filename\": \"textcleaner.py\", \"function\": \"<module>\", \"path\": \"/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/site-packages/gensim/summarization/textcleaner.py\", \"logger_name\": \"summarizer.preprocessing.cleaner\", \"lineno\": 37, \"process\": 82024, \"thread\": 140735962542976, \"details\": {}}\n"
     ]
    }
   ],
   "source": [
    "from factnlp.category.category_predictor import CategoryPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\": \"INFO\", \"message\": \"Loading models\", \"log_timestamp\": \"2018-08-25 18:32:57.963588\", \"filename\": \"tasks.py\", \"function\": \"models_loader_factory\", \"path\": \"/Users/rick/factmata/factmata-quality-engine/factnlp/distributor/tasks.py\", \"logger_name\": \"distributor.tasks\", \"lineno\": 53, \"process\": 82024, \"thread\": 140735962542976, \"details\": {\"message\": \"Loading models\"}}\n",
      "{\"level\": \"INFO\", \"message\": \"Loading models\", \"log_timestamp\": \"2018-08-25 18:32:57.966187\", \"filename\": \"tasks.py\", \"function\": \"models_loader_factory\", \"path\": \"/Users/rick/factmata/factmata-quality-engine/factnlp/distributor/tasks.py\", \"logger_name\": \"distributor.tasks\", \"lineno\": 53, \"process\": 82024, \"thread\": 140735962542976, \"details\": {\"message\": \"Loading models\"}}\n"
     ]
    }
   ],
   "source": [
    "import settings\n",
    "import distributor.tasks\n",
    "models = distributor.tasks.models_loader_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Google drive paths\n",
    "\n",
    "labelled_data_path = '/hyperpartisanship/CF labelled data'\n",
    "training_data_path = '/Users/mariarmestre/Projects/factnlp/models/hyperpartisanship/current/'\n",
    "\n",
    "output_data_path = '/hyperpartisanship/error analysis/with CF labelled data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\": \"INFO\", \"message\": \"Loading Politics Classifier Model\", \"log_timestamp\": \"2018-08-25 18:32:57.977813\", \"filename\": \"modelsloader.py\", \"function\": \"__load_hyperpartisanship_model\", \"path\": \"/Users/rick/factmata/factmata-quality-engine/factnlp/factnlp/modelsloader.py\", \"logger_name\": \"factnlp\", \"lineno\": 72, \"process\": 82024, \"thread\": 140735962542976, \"details\": {}}\n",
      "{\"level\": \"INFO\", \"message\": \"Loading Hyperpartisanship Model\", \"log_timestamp\": \"2018-08-25 18:33:41.400239\", \"filename\": \"modelsloader.py\", \"function\": \"__load_hyperpartisanship_model\", \"path\": \"/Users/rick/factmata/factmata-quality-engine/factnlp/factnlp/modelsloader.py\", \"logger_name\": \"factnlp\", \"lineno\": 74, \"process\": 82024, \"thread\": 140735962542976, \"details\": {}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<factnlp.hyperpartisanship.hyperpartisanship_predictor.HyperpartisanshipPredictor at 0x14e7fbba8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Production model\n",
    "\n",
    "production_hp = models.get_hyperpartisanship_model()\n",
    "production_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function to provide predict_proba() for LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ad hoc func to serve as predict_proba() for LIME\n",
    "import json\n",
    "classes = ['Biased','Unbiased']\n",
    "\n",
    "def fm_predict_proba(in_text_list,verbose=False):\n",
    "  \"\"\"\n",
    "  Takes a list of texts and for each gets a probability of being hyperpartisan from the hyperpartisan model\n",
    "  Then extends the probability returned into a tuple of (probability of true, probability of false) as required\n",
    "  by the LIME explain.explainer_instance function\n",
    "  \n",
    "  Args:\n",
    "    list of texts represented as strings - prediction will be run against each \n",
    "  \n",
    "  Returns:\n",
    "    numpy array of tuples - each represents (p(is hyperpartisan),p(is not hyperpartisan))\n",
    "  \"\"\"\n",
    "  \n",
    "  probs = []\n",
    "  if verbose:\n",
    "    print(\"base text length =\", len(in_text_list[0]))\n",
    "    print(\"fm_predict_proba - number of elements in text list \", len(in_text_list))\n",
    "  text_list = in_text_list\n",
    "  prob1 = prob2 = 0 \n",
    "  t0 = time.time()\n",
    "#   call to hyperpartisan classifier\n",
    "  results = production_hp.predict(text_list)\n",
    "  t1 = time.time()\n",
    "\n",
    "  if verbose:\n",
    "    print(\"predict run time = \",str(datetime.timedelta(seconds=round(t1-t0,0))))\n",
    "  for result in results :  \n",
    "    if result['class'] == classes[0]:\n",
    "      prob1 = result['score']\n",
    "      prob2 = 1 - prob1\n",
    "    else: \n",
    "      prob2 = result['score']\n",
    "      prob1 = 1 - prob2\n",
    "    probs.append([prob1,prob2])\n",
    "    \n",
    "  return(np.array(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.083, 0.917]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test if needed\n",
    "test_txt1 = []\n",
    "test_txt1.append(\"President Donald Trump was in the Philippines on Monday as part of the final stop on a whirlwind, 12-day tour of Asia that included warm receptions by the gracious hosts of Japan, South Korea, China and Vietnam, according to Fox News.\")\n",
    "res = fm_predict_proba(test_txt1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data, train a simple LR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import os.path\n",
    "\n",
    "test_file = \"/Users/rick/factmata/article_quotes.csv\"\n",
    "# test_file = \"/Users/rick/factmata/unit_test_quotes1.csv\"\n",
    "# print(\"file exists?\",os.path.isfile(test_file))\n",
    "\n",
    "training = pd.read_csv(\"/Users/rick/factmata/train.csv\")\n",
    "testing = pd.read_csv(test_file,encoding = \"ISO-8859-1\")\n",
    "class_names = ['Biased','Unbiased']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(training['text'])\n",
    "train_targets = training['tag']\n",
    "test_vectors = vectorizer.transform(testing['text'])\n",
    "test_targets = testing['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(C=1e5)\n",
    "lr.fit(train_vectors,train_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizer, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an explainer object \n",
    "Select text and run explainer\n",
    "Create list of explainer words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(split_expression=r'\\W+',class_names=class_names,ngram_size=2,stop_words=True,\n",
    "                             sentence_level=True)\n",
    "pred_proba_func = fm_predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose which model to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LR_MODEL = 0\n",
    "# FM_MODEL = 1 \n",
    "\n",
    "# # model_used = LR_MODEL\n",
    "# model_used = FM_MODEL\n",
    "\n",
    "# if model_used == FM_MODEL : \n",
    "#   pred_proba_func = fm_predict_proba\n",
    "# else :\n",
    "#   pred_proba_func = c.predict_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and index utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_quoted_regions(quoted_parts,randomize = False):\n",
    "    \"\"\"\n",
    "    sort quoted parts, then merge any which overlap or form\n",
    "    continuous quotes\n",
    "\n",
    "    Args :\n",
    "        list of tuples representing sections of text\n",
    "    Returns:\n",
    "        The same but with sections merged to continuous regions where\n",
    "        appropriate\n",
    "    \"\"\"\n",
    "    def purge_list(target_list, remove_list):\n",
    "        for i in reversed(remove_list):\n",
    "            del(target_list[i])\n",
    "        return target_list\n",
    "\n",
    "    remove_list = []\n",
    "    ####  remove any dodgy tuples ####\n",
    "    # check for tuples with start point after stop point, or negative start\n",
    "    for i in range(len(quoted_parts)):\n",
    "        start,stop = quoted_parts[i]\n",
    "        if (start < 0) or (start > stop):\n",
    "            print(\"warning - start point should be non-negative and not greater than stop point in \", quoted_parts[i])\n",
    "            remove_list.append(i)\n",
    "    # for i in reversed(remove_list):\n",
    "    #     del(quoted_parts[i])\n",
    "    purge_list(quoted_parts,remove_list)\n",
    "\n",
    "    #### main sort and merge ####\n",
    "    quoted_parts = sorted(quoted_parts)\n",
    "    quoted_parts_len = len(quoted_parts)\n",
    "    remove_list = []\n",
    "    for i in range(quoted_parts_len - 1):\n",
    "        # compare end of one part with start of other - merge if overlap\n",
    "        start1, stop1 = quoted_parts[i]\n",
    "        start2, stop2 = quoted_parts[i + 1]\n",
    "        if stop1 >= start2:\n",
    "            if stop2 <= stop1:\n",
    "                # remove subsumed part\n",
    "                remove_list.append(i+1)\n",
    "            else:\n",
    "                # second part absorbs first, first is removed\n",
    "                quoted_parts[i + 1] = (start1, stop2)\n",
    "                remove_list.append(i)\n",
    "    purge_list(quoted_parts,remove_list)\n",
    "\n",
    "    return quoted_parts\n",
    "  \n",
    "def randomize_regions(quoted_parts,text_len):\n",
    "  \"\"\"\n",
    "  Takes some quoted regions and returns randomized control regions of \n",
    "  the same length and number\n",
    "  \"\"\"\n",
    "  num_regions = len(quoted_parts)\n",
    "  num_rand_regions = 0 \n",
    "  rand_regions = []\n",
    "  attempts = 0\n",
    "  while (num_rand_regions < num_regions) and (attempts < 100):\n",
    "    start = random.randint(0,text_len-1)\n",
    "    (quote_start,quote_stop) = quoted_parts[num_rand_regions]\n",
    "    region_len = quote_stop - quote_start \n",
    "    stop = start + region_len \n",
    "    print(start,stop)\n",
    "    if  stop > text_len - 1 :\n",
    "      attempts += 1 \n",
    "      continue\n",
    "    else:\n",
    "      overlap = False\n",
    "      for i in range(num_rand_regions):\n",
    "        if (start in rand_regions[i]) or (stop in rand_regions[i]):\n",
    "          overlap = True\n",
    "          continue\n",
    "#   if there's an overlap don't keep this region , find a new one \n",
    "      if overlap:\n",
    "        continue\n",
    "      else:\n",
    "        rand_regions.append((start,stop))\n",
    "        num_rand_regions += 1\n",
    "  return rand_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "def get_quote_idxs(txt, quotes, err_file = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        txt - string\n",
    "        quotes - list of strings\n",
    "        err_file - optional file to record failures to match quote.\n",
    "    Returns:\n",
    "        list of tuples indicating (start,stop) position of each quote in txt (if doesn't appear it's ignored)\n",
    "        \n",
    "    \"\"\"\n",
    "    quoted_idxs = []\n",
    "    for quote in quotes:\n",
    "      start = txt.find(quote)\n",
    "      if start != -1 :\n",
    "        quoted_idxs.append((start,start+len(quote)))\n",
    "      else:\n",
    "        if err_file is not None:\n",
    "          err_file.write(\"\\n*START OF TEXT*\\n\")\n",
    "          err_file.write(txt)\n",
    "          err_file.write(\"\\n\")\n",
    "          err_file.write(\"** END OF TEXT*\\n\")\n",
    "          err_file.write(\"quote = \")\n",
    "          err_file.write(quote)\n",
    "          err_file.write(\"\\n***\\n\")\n",
    "        \n",
    "    return quoted_idxs\n",
    "\n",
    "\n",
    "##  function to return all rows (i.e. annotations) matching a page_id\n",
    "def get_quotes_list(testing, page_id):\n",
    "  \"\"\"\n",
    "   Args:\n",
    "     testing: panda data frame with text, quotes etc\n",
    "     page_id: id of piece of text (as represented in 'page__id' of data frame)\n",
    "   \n",
    "   Returns:\n",
    "     List of all rows in data frame that match page_id (i.e. all rows which represent\n",
    "     quotes from this piece of text)\n",
    "  \"\"\"\n",
    "\n",
    "#   TODO improve error handling ?\n",
    "#   if page_id not in testing['page__id']:\n",
    "#     return [] \n",
    "  page_refs = (testing['page__id'] == page_id)\n",
    "  pages_list = testing[page_refs]\n",
    "  return pages_list\n",
    "\n",
    "\n",
    "# def find_overlap(target_idxs, regions_idxs):\n",
    "#   overlap_start = 0 \n",
    "#   overlap_stop = 0 \n",
    "#   target_start,target_stop = target_idxs\n",
    "#   for reg_start,reg_stop in regions_idxs:\n",
    "#     if target_start >= reg_start:\n",
    "#       overlap_start = target_start\n",
    "#       if target_stop <= reg_stop:\n",
    "#         overlap_stop = target_stop\n",
    "#       else:\n",
    "#         overlap_stop = reg_stop\n",
    "#     else:                       # target_start before reg_start\n",
    "#       if target_stop > reg_start:\n",
    "#         overlap_start = reg_start\n",
    "#         if target_stop > reg_stop:\n",
    "#           overlap_stop = reg_stop\n",
    "#         else:\n",
    "#           overlap_stop = target_stop\n",
    "\n",
    "#   return(overlap_start,overlap_stop)\n",
    "\n",
    "def find_overlap(sentence, text, regions_idxs):\n",
    "  \n",
    "  if len(sentence) <= 0:\n",
    "    return(0)\n",
    "  overlap_start = 0 \n",
    "  overlap_stop = 0 \n",
    "  reg_idx_list = []\n",
    "  for start,stop in regions_idxs:\n",
    "    reg_idx_list.extend(range(start,stop))\n",
    "  overlaps = []\n",
    "  matched = False\n",
    "  start = -1\n",
    "\n",
    "#   target_idxs = get_quote_idxs(text,[sentence])\n",
    "  target_idxs = []\n",
    "  sub_string = text\n",
    "  loc = sub_string.find(sentence)\n",
    "  last_loc = 0\n",
    "#   allow for same sentence appearing more than once in text\n",
    "  time_to_die = 10 \n",
    "  while loc != -1:\n",
    "    loc_end = loc + len(sentence)\n",
    "    sub_string = sub_string[loc_end:]\n",
    "    target_idxs.append((last_loc + loc,last_loc + loc_end))\n",
    "    last_loc += loc_end\n",
    "    loc = sub_string.find(sentence)\n",
    "    time_to_die -= 1\n",
    "    if time_to_die < 0 :\n",
    "      print (\"find_overlap aborted - sentence =\\\"{}\\\",sub_string = \\\"{}\\\"\".format(sentence,sub_string))\n",
    "      break\n",
    "  if(target_idxs == []):\n",
    "    return(0)\n",
    "\n",
    "  for idxs in target_idxs:\n",
    "    start,stop = idxs\n",
    "    for targ_idx in range(start,stop+1):\n",
    "      if targ_idx in reg_idx_list:\n",
    "        if matched:\n",
    "          continue\n",
    "        else:\n",
    "          matched = True\n",
    "          start = targ_idx\n",
    "      else:\n",
    "        if matched:\n",
    "          overlaps.append((start,targ_idx))\n",
    "          start = -1\n",
    "          matched = False\n",
    "    # tie up loose end, if stop mid region or at end of buffer\n",
    "    if matched:\n",
    "      # for tidiness avoid keeping eg. (10,10) when target buffer stops immediately before region starts\n",
    "      if targ_idx > start: \n",
    "        overlaps.append((start,targ_idx))\n",
    "      \n",
    "#   print(\"target_idxs\", target_idxs)\n",
    "  target_len = sum([target_idx[1] - target_idx[0] for target_idx in target_idxs])\n",
    "  overlap_total = sum([max(0,stop - start) for start,stop in overlaps])\n",
    "  score = overlap_total/target_len\n",
    "  return(score)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "def get_word_scores(word_list, text, text_as_dict, quotes_region): \n",
    "  \"\"\"\n",
    "  Produces score on how often the words appear in the text and in the quotes region\n",
    "  (Simpler version of get_word_stats, which uses a list of quotes rather than unified\n",
    "  quote regions)\n",
    "\n",
    "  Args:\n",
    "    words_list: a subset of words or ngrams from the text \n",
    "    text : the text itself\n",
    "    text_as_dict:  dict with key as a word and value its occurence count e.g {'fall':3}\n",
    "    quotes_region : list of tuples in form (start, stop) indicating position of quote regions in text\n",
    "\n",
    "  Returns: Dict - key is word, value is dict with annt_count - no of occurences of word in quote region\n",
    "                                                  text_count - no of occurences of word in entire text \n",
    "                                \n",
    "  \"\"\"\n",
    "  \n",
    "  debug_list = [(start,stop) for (start,stop) in quotes_region]\n",
    "#   print(\"quotes region = \", sorted(quotes_region))\n",
    "  all_quotes = ' '.join([text[start:stop] for (start,stop) in quotes_region])\n",
    "  all_quotes_count_dict = Counter(np.array(re.split(r'%s|$' % r'\\W+' , all_quotes)))\n",
    "  scores = {word:{'annt_count':0,\n",
    "                 'text_count':0,\n",
    "                  'weight':weight\n",
    "                } for (word,weight) in word_list}\n",
    "  for word,_ in word_list:  \n",
    "    scores[word]['text_count'] = text_as_dict[word]\n",
    "    scores[word]['annt_count'] = all_quotes_count_dict[word]    \n",
    "  return scores\n",
    "\n",
    "##################################################################\n",
    "# func - get_ngram_scores\n",
    "##################################################################\n",
    "\n",
    "def get_ngram_scores(word_list, text, text_as_dict, quotes_region,n=2): \n",
    "  \"\"\"\n",
    "  Produces score on how often the ngrams appear in the text and in the quotes region\n",
    "\n",
    "  Args:\n",
    "    words: a subset of words or ngrams from the text (ngrams must be represented as tuples of word strings)\n",
    "            e.g. (\"banana\",\"split\")\n",
    "    text : the text itself\n",
    "    text_as_dict:  dict with key as a word/ngram and value its occurence count e.g {'fall':3}\n",
    "    quotes_region : list of tuples in form (start, stop) indicating position of quote regions in text\n",
    "    n : size of ngram \n",
    "\n",
    "  Returns: Dict - key is word, value is dict with annt_count - no of occurences of word in quote region\n",
    "                                                  text_count - no of occurences of word in entire text \n",
    "                                \n",
    "  \"\"\"\n",
    "\n",
    "#   dict of ngram counts in qoute regions\n",
    "  all_quotes_count_dict = Counter()\n",
    "  \n",
    "#   split , removing any non-word characters, rejoin, form and count ngrams\n",
    "  for (start,stop) in quotes_region:\n",
    "    all_quotes_count_dict.update(ngrams(re.split(r'%s|$' % r'\\W+' , text[start:stop]),n))\n",
    "      \n",
    "# convert ngram to str, so can use as JSON key\n",
    "  scores = {' '.join(ngram):{'annt_count':all_quotes_count_dict[ngram],\n",
    "                             'text_count':text_as_dict[ngram],\n",
    "                             'weight':weight\n",
    "                } for (ngram, weight) in word_list}\n",
    "#   for word,_ in word_list:  \n",
    "#     scores[word]['text_count'] = text_as_dict[word]\n",
    "#     scores[word]['annt_count'] = all_quotes_count_dict[word]    \n",
    "  return scores\n",
    "\n",
    "\n",
    "def display_stats(exp_words, stats):\n",
    "  for word in exp_words:\n",
    "    if stats[word]['annt_count'] != 0 :\n",
    "      word_stat = stats[word]\n",
    "      idf = word_stat['idf']\n",
    "    if stats[word]['annt_freq'] > stats[word]['text_freq'] :\n",
    "      print(\"%20s >>> text-freq = %.5f  quote-freq = \\x1b[31m%.5f\\x1b[0m\"%(word, stats[word]['text_freq'] , stats[word]['annt_freq']))\n",
    "    else:\n",
    "      print(\"%20s >>> text-freq = %.5f  quote-freq = %.5f\"%(word, stats[word]['text_freq'] , stats[word]['annt_freq']))\n",
    "      \n",
    "  print('\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_scores(sentence_list, text, text_as_dict, quotes_region): \n",
    "  \"\"\"\n",
    "  Produces score on how often the words appear in the text and in the quotes region\n",
    "  (Simpler version of get_word_stats, which uses a list of quotes rather than unified\n",
    "  quote regions)\n",
    "\n",
    "  Args:\n",
    "    sent_list: a subset of sentences from the text, as list of strings \n",
    "    text : the text itself\n",
    "    text_as_dict:  dict with key as a sentence and value its occurence count e.g {'fall':3}\n",
    "    quotes_region : list of tuples in form (start, stop) indicating position of quote regions in text\n",
    "\n",
    "  Returns: Dict - key is sentence, value is dict with annt_count - no of occurences of word in quote region\n",
    "                                                  text_count - no of occurences of word in entire text \n",
    "                  \n",
    "  \"\"\"\n",
    "    \n",
    "  scores = {sentence:{'annt_count':find_overlap(sentence, text, quotes_region),\n",
    "                 'text_count':text_as_dict[sentence],\n",
    "                  'weight':weight\n",
    "                } for (sentence,weight) in sentence_list}\n",
    "  return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          quote\n",
      "0  A short sentence. the quick!\n",
      "1                   jumped over\n",
      "2                     brown fox\n",
      "A short sentence. the quick! jumped over.....the rest is history. the quic??holy\n",
      "text dictionary\n",
      "Counter({'.': 2, 'A short sentence': 1, ' the quick': 1, '!': 1, ' jumped over': 1, '.....': 1, 'the rest is history': 1, ' the quic': 1, '??': 1, 'holy': 1})\n",
      "quote region [(0, 28), (29, 40)]\n",
      "target_idxs [(17, 27)]\n",
      "target_idxs [(45, 64)]\n",
      " the quick               {'annt_count': 1.0, 'text_count': 1, 'weight': 0.4}\n",
      "the rest is history      {'annt_count': 0.0, 'text_count': 1, 'weight': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#  #  #  # #  #  # #  #  # #  #  # #  #  # #  #  #\n",
    "#   #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #\n",
    "# test code for get_sentence_score()\n",
    "\n",
    "# create simple panda data frame\n",
    "col_names =  ['quote']\n",
    "quotes_df  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "quotes_df.loc[0] = [\"A short sentence. the quick!\"]\n",
    "quotes_df.loc[1] = [\"jumped over\"]\n",
    "quotes_df.loc[2] = [\"brown fox\"]\n",
    "# quotes_df.loc[3] = ['\" the quick??holy\"']\n",
    "print(quotes_df)\n",
    "\n",
    "text = \"A short sentence. the quick! jumped over.....the rest is history. the quic??holy\"\n",
    "print(text)\n",
    "n = 2 \n",
    "text_dict = Counter(np.array(re.split(r'([.!?]+)|$', text)))#*BK1\n",
    "\n",
    "print(\"text dictionary\")\n",
    "print(text_dict)\n",
    "\n",
    "sent_list = [((\" the quick\"),0.4),((\"the rest is history\"),0.2)]\n",
    "\n",
    "quotes_list = [quote for quote in quotes_df['quote']]\n",
    "test_quote_idxs = get_quote_idxs(text,quotes_list)\n",
    "\n",
    "quote_region = build_quoted_regions(test_quote_idxs)\n",
    "print(\"quote region\", quote_region)\n",
    "my_score = get_sentence_scores(sent_list, text, text_dict, quote_region)\n",
    "for key in my_score.keys():\n",
    "  print('{:25}{}'.format(key,my_score[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5833333333333334\n",
      "1 0\n",
      "2 0\n",
      "3 0.2857142857142857\n",
      "4 0.47619047619047616\n",
      "5 0.375\n",
      "6 1.0\n",
      "7 0\n",
      "-end of expected-\n",
      "target_idxs [(23, 35)]\n",
      "0 0.5833333333333334 n I'm straig\n",
      "target_idxs [(15, 19)]\n",
      "1 0.0 ning\n",
      "2 0 567890\n",
      "target_idxs [(0, 7)]\n",
      "3 0.2857142857142857 Some ve\n",
      "target_idxs [(4, 25)]\n",
      "4 0.47619047619047616  velvet morning when \n",
      "target_idxs [(0, 40)]\n",
      "5 0.375 Some velvet morning when I'm straight I'\n",
      "target_idxs [(29, 30)]\n",
      "6 1.0 s\n",
      "target_idxs [(35, 42)]\n",
      "7 0.0 ht I'm \n",
      "target_idxs [(25, 28), (38, 41)]\n",
      "8 0.5 I'm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###  #  #  # #  #  # #  #  # #  #  # #  #  # #  #  #\n",
    "#   #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #\n",
    "# test code for find_overlap\n",
    "\n",
    "regions = [(5,10),(20,30)]\n",
    "sent_idxs = [(23,35),(15,19),(15,20),(0,7),(4,25),(0,40),(29,30),(35,40)]\n",
    "# expect \n",
    "# 0(23,30)\n",
    "# 1(0,0)\n",
    "# 2(0,0)\n",
    "# 3(5,7)\n",
    "# 4(5,10)(20,25)\n",
    "# 5(5,10)(20,30)\n",
    "# 6 (29,30)\n",
    "# 7 (0,0)\n",
    "# expect\n",
    "print(\"0\",7/12)\n",
    "print(\"1\",0)\n",
    "print(\"2\",0)\n",
    "print(\"3\",2/7)\n",
    "print(\"4\",10/21)\n",
    "print(\"5\",15/40)\n",
    "print(\"6\",1/1)\n",
    "print(\"7\",0)\n",
    "print(\"-end of expected-\")\n",
    "\n",
    "text = \"Some velvet morning when I'm straight I'm going to walk up to your gate\"\n",
    "#       01234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
    "#       0---------10--------20--------30--------40--------50--------10--------10--------\n",
    "for i,si in enumerate([\"n I'm straig\",\"ning\",\"567890\",\"Some ve\",\" velvet morning when \",\"Some velvet morning when I'm straight I'\",\"s\",\"ht I'm \",\"I'm\"]):\n",
    "  score = find_overlap(si,text,regions)\n",
    "  print(i, score,si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         quote\n",
      "0    the quick\n",
      "1  jumped over\n",
      "2    brown fox\n",
      "the quick brown fox jumped over the lazy dog more dog rhubarb rhubarb dog rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarbrhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb\n",
      "text dictionary\n",
      "Counter({('rhubarb', 'rhubarb'): 49, ('dog', 'rhubarb'): 2, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumped'): 1, ('jumped', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', 'more'): 1, ('more', 'dog'): 1, ('rhubarb', 'dog'): 1, ('rhubarb', 'rhubarbrhubarb'): 1, ('rhubarbrhubarb', 'rhubarb'): 1})\n",
      "quote region [(0, 9), (10, 19), (20, 31)]\n",
      "{'the quick': {'annt_count': 1, 'text_count': 1, 'weight': 0.4}, 'lazy fox': {'annt_count': 0, 'text_count': 0, 'weight': 0.2}, 'brown fox': {'annt_count': 1, 'text_count': 1, 'weight': 0.3}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#  #  #  # #  #  # #  #  # #  #  # #  #  # #  #  #\n",
    "#   #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #\n",
    "# test code for get_ngram_score()\n",
    "\n",
    "# create simple panda data frame\n",
    "col_names =  ['quote']\n",
    "quotes_df  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "quotes_df.loc[0] = [\"the quick\"]\n",
    "quotes_df.loc[1] = [\"jumped over\"]\n",
    "quotes_df.loc[2] = [\"brown fox\"]\n",
    "print(quotes_df)\n",
    "\n",
    "text = 'the quick brown fox jumped over the lazy dog more dog rhubarb rhubarb dog rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarbrhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb'\n",
    "print(text)\n",
    "n = 2 \n",
    "text_dict = Counter(ngrams(re.split(r'%s|$' % r'\\W+' , text),n))\n",
    "print(\"text dictionary\")\n",
    "print(text_dict)\n",
    "\n",
    "words_list = [((\"the\" ,\"quick\"),0.4),((\"lazy\",\"fox\"),0.2),((\"brown\",\"fox\"),0.3)]\n",
    "\n",
    "quotes_list = [quote for quote in quotes_df['quote']]\n",
    "test_quote_idxs = get_quote_idxs(text,quotes_list)\n",
    "\n",
    "quote_region = build_quoted_regions(test_quote_idxs)\n",
    "print(\"quote region\", quote_region)\n",
    "my_score = get_ngram_scores(words_list, text, text_dict, quote_region)\n",
    "print(my_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the quick': {'annt_count': 1, 'text_count': 1, 'weight': 0.4},\n",
       " 'lazy fox': {'annt_count': 0, 'text_count': 0, 'weight': 0.2},\n",
       " 'brown fox': {'annt_count': 1, 'text_count': 1, 'weight': 0.3}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         quote\n",
      "0     lazy dog\n",
      "1  jumped over\n",
      "text dictionary\n",
      "Counter({'rhubarb': 52, 'dog': 3, 'the': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumped': 1, 'over': 1, 'lazy': 1, 'more': 1, 'rhubarbrhubarb': 1})\n",
      "quote region [(20, 31), (36, 44)]\n",
      "{'dog': {'annt_count': 1, 'text_count': 3, 'weight': 0.4}, 'rhubarb': {'annt_count': 0, 'text_count': 52, 'weight': 0.2}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# test code \n",
    "\n",
    "col_names =  ['quote']\n",
    "quotes_df  = pd.DataFrame(columns = col_names)\n",
    "# my_df\n",
    "# If you want to add a record to the dataframe it would be better to use:\n",
    "\n",
    "# quotes_df.loc[len(my_df)] = [\"lazy dog\"]\n",
    "quotes_df.loc[0] = [\"lazy dog\"]\n",
    "quotes_df.loc[1] = [\"jumped over\"]\n",
    "# quotes_df.loc[1] = [\"quick fox\"]\n",
    "print(quotes_df)\n",
    "\n",
    "text = 'the quick brown fox jumped over the lazy dog more dog rhubarb rhubarb dog rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarbrhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb'\n",
    "text_dict = Counter(np.array(re.split(r'%s|$' % r'\\W+' , text)))\n",
    "print(\"text dictionary\")\n",
    "print(text_dict)\n",
    "\n",
    "words_list = [(\"dog\",0.4),(\"rhubarb\",0.2)]\n",
    "\n",
    "quotes_list = [quote for quote in quotes_df['quote']]\n",
    "test_quote_idxs = get_quote_idxs(text,quotes_list)\n",
    "\n",
    "\n",
    "quote_region = build_quoted_regions(test_quote_idxs)\n",
    "print(\"quote region\", quote_region)\n",
    "my_score = get_word_scores(words_list, text, text_dict, quote_region)\n",
    "print(my_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create sorted list of individual page__id's\n",
    "page_id_set = set(testing['page__id'])\n",
    "page_id_list = list(page_id_set)\n",
    "page_id_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n",
      "/Users/rick/.pyenv/versions/3.6.1/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# LIME parameters\n",
    "num_features = 3\n",
    "\n",
    "# experiment parameters\n",
    "debug_display = False\n",
    "MIN_TXT_SIZE = 500\n",
    "# rand_control_num = 500\n",
    "rand_control_num = 20\n",
    "num_samples = 5000 # number of samples LIME uses to build it's model\n",
    "stop_words = True # True - stop words are used ; False - stop words are removed  \n",
    "\n",
    "num_articles = len(page_id_list)\n",
    "\n",
    "def open_results_files(ts):\n",
    "  ts = time.localtime()\n",
    "  results_file = \"results/\" + time.strftime(\"%d%m%Y_%H%M%S_S_results.json\", ts)\n",
    "  errors_file = \"results/errors/\" + time.strftime(\"%d%m%Y_%H%M%S_S_errors.txt\", ts)\n",
    "  try:\n",
    "    fr = open(results_file,'w')\n",
    "  except:\n",
    "    print(\"error opening file %s\" % (results_file))\n",
    "  try:\n",
    "    fe = open(errors_file,'w')\n",
    "  except:\n",
    "    print(\"error opening file %s\" % (errors_file)) \n",
    "  return fr, fe \n",
    "\n",
    "# def run_experiment(num_articles, num_features,num_samples)\n",
    "# ********** MAIN LOOP *************\n",
    "# look at first [num_articles] pages\n",
    "all_word_scores = []\n",
    "all_random_word_scores = []\n",
    "\n",
    "ts = time.localtime()\n",
    "timestamp =  time.strftime(\"%H%M%S_%d%m%Y\", ts)\n",
    "\n",
    "desc_string = \"sentence level - num features 3\"\n",
    "short_desc = \"sentences v003\"\n",
    "num_trials = 1\n",
    "\n",
    "\n",
    "#  note - JSON keys have to be strings (unlike python dict keys)\n",
    "experiment_dict = {'timestamp':timestamp,\n",
    "                   'description': desc_string ,\n",
    "                   'short_desc' : short_desc,\n",
    "                     'trials':\n",
    "                       {str(trial):\n",
    "                         {\n",
    "                           'num_articles':num_articles,\n",
    "                           'num_features':num_features,\n",
    "                           'num_samples':num_samples,\n",
    "                           'rand_sample_size':rand_control_num,\n",
    "                           'stop_words': stop_words,\n",
    "                           'corpus':test_file,\n",
    "                           'run_time':'',\n",
    "                           'articles':\n",
    "                             {str(page_id):\n",
    "                               {\n",
    "                               'score':0.0,\n",
    "                               'txt_len':0,\n",
    "                               'quote_len':0,\n",
    "                               'explainer_words':[],\n",
    "                               'random_words':[],\n",
    "                               'bbm_score':0.0\n",
    "                               } for page_id in page_id_list[:num_articles]                    \n",
    "                             }\n",
    "                         }for trial in range(num_trials)\n",
    "                       }\n",
    "                  }\n",
    "                    \n",
    "\n",
    "res_file, err_file = open_results_files(ts)\n",
    "\n",
    "\n",
    "page_dict = {}\n",
    "# how many times to select random word comparision set from same text \n",
    "\n",
    "trial_key = str(0)\n",
    "# for page_id in page_id_list[:num_articles]:\n",
    "# for page_id in page_id_list:\n",
    "###################################################\n",
    "## LOOP THROUGH ARTICLES                         ##\n",
    "###################################################\n",
    "# for page_id in page_id_list:\n",
    "for page_id in page_id_list[:1]:\n",
    "  page_key = str(page_id)\n",
    "  print(\"page id = \",page_id)\n",
    "#  get text of article\n",
    "  text = testing[testing['page__id'] == page_id].iloc[0]['text']\n",
    "  text = text.lower()\n",
    "\n",
    "#   skip to next article, if article text is too small\n",
    "  if len(text) < MIN_TXT_SIZE:\n",
    "    continue\n",
    "\n",
    "  #   Run LIME explainer to get explainer words\n",
    "  exp = explainer.explain_instance(text, pred_proba_func, num_features=num_features, num_samples=num_samples)\n",
    "  if debug_display:\n",
    "    print('Probability(biased) =', c.predict_proba([text])[0,0])\n",
    "  experiment_dict['trials'][trial_key]['articles'][page_key]['score'] = exp.score\n",
    "  exp_words = [word for (word,weight) in exp.as_list()]\n",
    "\n",
    "  # find all quotes for given article\n",
    "  quotes_list_df = get_quotes_list(testing, page_id)\n",
    "  quotes_list = [quote.lower() for quote in quotes_list_df['quote']]\n",
    "  quotes_idxs = get_quote_idxs(text, quotes_list, err_file)\n",
    "  quote_regions = build_quoted_regions(quotes_idxs)\n",
    "  \n",
    "  # r'[.!?]+|$' instead of r'([.!?]+)|$' means matched separators not included in list   \n",
    "  text_as_sentences = np.array(re.split(r'[.!?]+|$', text))\n",
    "#   if end of text matches split criteria, it will include an empty string\n",
    "#   text_as_sentences = np.array(filter(None,text_as_sentences))\n",
    "  \n",
    "  text_count_dict = Counter(text_as_sentences) \n",
    "                            \n",
    "# don't filter out stop words\n",
    "  \n",
    "  #   LIME WORDS\n",
    "  sentence_scores = get_sentence_scores(exp.as_list(), text, \n",
    "                                        text_count_dict, quote_regions )\n",
    "  experiment_dict['trials'][trial_key]['articles'][page_key]['explainer_words'] = sentence_scores\n",
    "\n",
    "  #   RANDOM WORDS\n",
    "  #   control group of bigrams randomly selected from article    \n",
    "  for i in range(rand_control_num):\n",
    "    rand_choice = np.random.choice(text_as_sentences.size, num_features, replace=False)\n",
    "    random_sentences = [(random,0.0) for random in text_as_sentences[rand_choice]]\n",
    "    if debug_display:\n",
    "      print(random_sentences)\n",
    "    random_sent_scores = get_sentence_scores(random_sentences, text, text_count_dict, quote_regions)\n",
    "    experiment_dict['trials'][trial_key]['articles'][page_key]['random_words'].append(random_sent_scores)\n",
    "\n",
    "json.dump(experiment_dict,res_file)\n",
    "res_file.close()\n",
    "err_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
