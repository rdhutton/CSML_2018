{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03-JUL-18\n",
    "\n",
    "Adaptation of \"LIME - basic LR\"\n",
    "\n",
    "Trains on the hyperpartisan data\n",
    "\n",
    "Tests on annotated data from Briefr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/rick/factmata/factnlp-experimental/lime')\n",
    "\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/rick/factmata/factmata-quality-engine/factnlp')\n",
    "sys.path.append('/Users/rick/factmata/utils')\n",
    "sys.path.append('/Users/rick/factmata/fastText')\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from factnlp.category.category_predictor import CategoryPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 21:17:50,413 | INFO | tasks.py-0080 | Loading models\n"
     ]
    }
   ],
   "source": [
    "import settings\n",
    "import distributor.tasks\n",
    "models = distributor.tasks.models_loader_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Google drive paths\n",
    "\n",
    "labelled_data_path = '/hyperpartisanship/CF labelled data'\n",
    "training_data_path = '/Users/mariarmestre/Projects/factnlp/models/hyperpartisanship/current/'\n",
    "\n",
    "output_data_path = '/hyperpartisanship/error analysis/with CF labelled data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 21:17:50,486 | INFO | modelsloader.py-0068 | Loading Politics Classifier Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator Pipeline from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 21:20:00,658 | INFO | modelsloader.py-0070 | Loading Hyperpartisanship Model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<factnlp.hyperpartisanship.hyperpartisanship_predictor.HyperpartisanshipPredictor at 0x1251cd828>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Production model\n",
    "\n",
    "production_hp = models.get_hyperpartisanship_model()\n",
    "production_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function to provide predict_proba() for LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ad hoc func to serve as predict_proba() for LIME\n",
    "import json\n",
    "classes = ['Biased','Unbiased']\n",
    "\n",
    "def fm_predict_proba(in_text_list,verbose=False):\n",
    "  \"\"\"\n",
    "  Takes a list of texts and for each gets a probability of being hyperpartisan from the hyperpartisan model\n",
    "  Then extends the probability returned into a tuple of (probability of true, probability of false) as required\n",
    "  by the LIME explain.explainer_instance function\n",
    "  \n",
    "  Args:\n",
    "    list of texts represented as strings - prediction will be run against each \n",
    "  \n",
    "  Returns:\n",
    "    numpy array of tuples - each represents (p(is hyperpartisan),p(is not hyperpartisan))\n",
    "  \"\"\"\n",
    "  \n",
    "  probs = []\n",
    "  if verbose:\n",
    "    print(\"base text length =\", len(in_text_list[0]))\n",
    "    print(\"fm_predict_proba - number of elements in text list \", len(in_text_list))\n",
    "  text_list = in_text_list\n",
    "  prob1 = prob2 = 0 \n",
    "  t0 = time.time()\n",
    "#   call to hyperpartisan classifier\n",
    "  results = production_hp.predict(text_list)\n",
    "  t1 = time.time()\n",
    "\n",
    "  if verbose:\n",
    "    print(\"predict run time = \",str(datetime.timedelta(seconds=round(t1-t0,0))))\n",
    "  for result in results :  \n",
    "    if result['class'] == classes[0]:\n",
    "      prob1 = result['score']\n",
    "      prob2 = 1 - prob1\n",
    "    else: \n",
    "      prob2 = result['score']\n",
    "      prob1 = 1 - prob2\n",
    "    probs.append([prob1,prob2])\n",
    "    \n",
    "  return(np.array(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00235569, 0.99764431]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test if needed\n",
    "test_txt1 = []\n",
    "test_txt1.append(\"President Donald Trump was in the Philippines on Monday as part of the final stop on a whirlwind, 12-day tour of Asia that included warm receptions by the gracious hosts of Japan, South Korea, China and Vietnam, according to Fox News.\")\n",
    "res = fm_predict_proba(test_txt1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data, train a simple LR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "test_file = \"/Users/rick/factmata/article_quotes.csv\"\n",
    "\n",
    "training = pd.read_csv(\"/Users/rick/factmata/train.csv\")\n",
    "testing = pd.read_csv(test_file)\n",
    "class_names = ['Biased','Unbiased']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(training['text'])\n",
    "train_targets = training['tag']\n",
    "test_vectors = vectorizer.transform(testing['text'])\n",
    "test_targets = testing['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(C=1e5)\n",
    "lr.fit(train_vectors,train_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizer, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an explainer object \n",
    "Select text and run explainer\n",
    "Create list of explainer words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(split_expression=r'\\W+',class_names=class_names,ngram_size=2,stop_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose which model to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_MODEL = 0\n",
    "FM_MODEL = 1 \n",
    "\n",
    "# model_used = LR_MODEL\n",
    "model_used = FM_MODEL\n",
    "\n",
    "if model_used == FM_MODEL : \n",
    "  pred_proba_func = fm_predict_proba\n",
    "else :\n",
    "  pred_proba_func = c.predict_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and index utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_quoted_regions(quoted_parts,randomize = False):\n",
    "    \"\"\"\n",
    "    sort quoted parts, then merge any which overlap or form\n",
    "    continuous quotes\n",
    "\n",
    "    Args :\n",
    "        list of tuples representing sections of text\n",
    "    Returns:\n",
    "        The same but with sections merged to continuous regions where\n",
    "        appropriate\n",
    "    \"\"\"\n",
    "    def purge_list(target_list, remove_list):\n",
    "        for i in reversed(remove_list):\n",
    "            del(target_list[i])\n",
    "        return target_list\n",
    "\n",
    "    remove_list = []\n",
    "    ####  remove any dodgy tuples ####\n",
    "    # check for tuples with start point after stop point, or negative start\n",
    "    for i in range(len(quoted_parts)):\n",
    "        start,stop = quoted_parts[i]\n",
    "        if (start < 0) or (start > stop):\n",
    "            print(\"warning - start point should be non-negative and not greater than stop point in \", quoted_parts[i])\n",
    "            remove_list.append(i)\n",
    "    # for i in reversed(remove_list):\n",
    "    #     del(quoted_parts[i])\n",
    "    purge_list(quoted_parts,remove_list)\n",
    "\n",
    "    #### main sort and merge ####\n",
    "    quoted_parts = sorted(quoted_parts)\n",
    "    quoted_parts_len = len(quoted_parts)\n",
    "    remove_list = []\n",
    "    for i in range(quoted_parts_len - 1):\n",
    "        # compare end of one part with start of other - merge if overlap\n",
    "        start1, stop1 = quoted_parts[i]\n",
    "        start2, stop2 = quoted_parts[i + 1]\n",
    "        if stop1 >= start2:\n",
    "            if stop2 <= stop1:\n",
    "                # remove subsumed part\n",
    "                remove_list.append(i+1)\n",
    "            else:\n",
    "                # second part absorbs first, first is removed\n",
    "                quoted_parts[i + 1] = (start1, stop2)\n",
    "                remove_list.append(i)\n",
    "    purge_list(quoted_parts,remove_list)\n",
    "\n",
    "    return quoted_parts\n",
    "  \n",
    "def randomize_regions(quoted_parts,text_len):\n",
    "  \"\"\"\n",
    "  Takes some quoted regions and returns randomized control regions of \n",
    "  the same length and number\n",
    "  \"\"\"\n",
    "  num_regions = len(quoted_parts)\n",
    "  num_rand_regions = 0 \n",
    "  rand_regions = []\n",
    "  attempts = 0\n",
    "  while (num_rand_regions < num_regions) and (attempts < 100):\n",
    "    start = random.randint(0,text_len-1)\n",
    "    (quote_start,quote_stop) = quoted_parts[num_rand_regions]\n",
    "    region_len = quote_stop - quote_start \n",
    "    stop = start + region_len \n",
    "    print(start,stop)\n",
    "    if  stop > text_len - 1 :\n",
    "      attempts += 1 \n",
    "      continue\n",
    "    else:\n",
    "      overlap = False\n",
    "      for i in range(num_rand_regions):\n",
    "        if (start in rand_regions[i]) or (stop in rand_regions[i]):\n",
    "          overlap = True\n",
    "          continue\n",
    "#   if there's an overlap don't keep this region , find a new one \n",
    "      if overlap:\n",
    "        continue\n",
    "      else:\n",
    "        rand_regions.append((start,stop))\n",
    "        num_rand_regions += 1\n",
    "  return rand_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "def get_quote_idxs(txt, quotes, err_file = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        txt - string\n",
    "        quotes - list of string\n",
    "        err_file - optional file to record failures to match quote.\n",
    "    Returns:\n",
    "        list of tuples indicating (start,stop) position of each quote in txt (if doesn't appear it's ignored)\n",
    "        \n",
    "    \"\"\"\n",
    "    quoted_idxs = []\n",
    "    for quote in quotes:\n",
    "      start = txt.find(quote)\n",
    "      if start != -1 :\n",
    "        quoted_idxs.append((start,start+len(quote)))\n",
    "      else:\n",
    "        if err_file is not None:\n",
    "          err_file.write(\"\\n*START OF TEXT*\\n\")\n",
    "          err_file.write(txt)\n",
    "          err_file.write(\"\\n\")\n",
    "          err_file.write(\"** END OF TEXT*\\n\")\n",
    "          err_file.write(\"quote = \")\n",
    "          err_file.write(quote)\n",
    "          err_file.write(\"\\n***\\n\")\n",
    "        \n",
    "    return quoted_idxs\n",
    "\n",
    "\n",
    "##  function to return all rows (i.e. annotations) matching a page_id\n",
    "def get_quotes_list(testing, page_id):\n",
    "  \"\"\"\n",
    "   Args:\n",
    "     testing: panda data frame with text, quotes etc\n",
    "     page_id: id of piece of text (as represented in 'page__id' of data frame)\n",
    "   \n",
    "   Returns:\n",
    "     List of all rows in data frame that match page_id (i.e. all rows which represent\n",
    "     quotes from this piece of text)\n",
    "  \"\"\"\n",
    "\n",
    "#   TODO improve error handling ?\n",
    "#   if page_id not in testing['page__id']:\n",
    "#     return [] \n",
    "  page_refs = (testing['page__id'] == page_id)\n",
    "  pages_list = testing[page_refs]\n",
    "  return pages_list\n",
    "\n",
    "def get_word_scores(word_list, text, text_as_dict, quotes_region): \n",
    "  \"\"\"\n",
    "  Produces score on how often the words appear in the text and in the quotes region\n",
    "  (Simpler version of get_word_stats, which uses a list of quotes rather than unified\n",
    "  quote regions)\n",
    "\n",
    "  Args:\n",
    "    words: a subset of words or ngrams from the text \n",
    "    text : the text itself\n",
    "    text_as_dict:  dict with key as a word and value its occurence count e.g {'fall':3}\n",
    "    quotes_region : list of tuples in form (start, stop) indicating position of quote regions in text\n",
    "    n : size of ngram - e.g 2 is bigram, 3 is trigram etc\n",
    "\n",
    "  Returns: Dict - key is word, value is dict with annt_count - no of occurences of word in quote region\n",
    "                                                  text_count - no of occurences of word in entire text \n",
    "                                \n",
    "  \"\"\"\n",
    "  \n",
    "  debug_list = [(start,stop) for (start,stop) in quotes_region]\n",
    "#   print(\"quotes region = \", sorted(quotes_region))\n",
    "  all_quotes = ' '.join([text[start:stop] for (start,stop) in quotes_region])\n",
    "  all_quotes_count_dict = Counter(np.array(re.split(r'%s|$' % r'\\W+' , all_quotes)))\n",
    "  scores = {word:{'annt_count':0,\n",
    "                 'text_count':0,\n",
    "                  'weight':weight\n",
    "                } for (word,weight) in word_list}\n",
    "  for word,_ in word_list:  \n",
    "    scores[word]['text_count'] = text_as_dict[word]\n",
    "    scores[word]['annt_count'] = all_quotes_count_dict[word]    \n",
    "  return scores\n",
    "\n",
    "##################################################################\n",
    "# func - get_ngram_scores\n",
    "##################################################################\n",
    "\n",
    "def get_ngram_scores(word_list, text, text_as_dict, quotes_region,n=2): \n",
    "  \"\"\"\n",
    "  Produces score on how often the ngrams appear in the text and in the quotes region\n",
    "\n",
    "  Args:\n",
    "    words: a subset of words or ngrams from the text (ngrams must be represented as tuples of word strings)\n",
    "            e.g. (\"banana\",\"split\")\n",
    "    text : the text itself\n",
    "    text_as_dict:  dict with key as a word/ngram and value its occurence count e.g {'fall':3}\n",
    "    quotes_region : list of tuples in form (start, stop) indicating position of quote regions in text\n",
    "    n : size of ngram \n",
    "\n",
    "  Returns: Dict - key is word, value is dict with annt_count - no of occurences of word in quote region\n",
    "                                                  text_count - no of occurences of word in entire text \n",
    "                                \n",
    "  \"\"\"\n",
    "\n",
    "#   dict of ngram counts in qoute regions\n",
    "  all_quotes_count_dict = Counter()\n",
    "  \n",
    "#   split , removing any non-word characters, rejoin, form and count ngrams\n",
    "  for (start,stop) in quotes_region:\n",
    "    all_quotes_count_dict.update(ngrams(re.split(r'%s|$' % r'\\W+' , text[start:stop]),n))\n",
    "      \n",
    "# convert ngram to str, so can use as JSON key\n",
    "  scores = {' '.join(ngram):{'annt_count':all_quotes_count_dict[ngram],\n",
    "                 'text_count':text_as_dict[ngram],\n",
    "                  'weight':weight\n",
    "                } for (ngram,weight) in word_list}\n",
    "#   for word,_ in word_list:  \n",
    "#     scores[word]['text_count'] = text_as_dict[word]\n",
    "#     scores[word]['annt_count'] = all_quotes_count_dict[word]    \n",
    "  return scores\n",
    "\n",
    "\n",
    "def display_stats(exp_words, stats):\n",
    "  for word in exp_words:\n",
    "    if stats[word]['annt_count'] != 0 :\n",
    "      word_stat = stats[word]\n",
    "      idf = word_stat['idf']\n",
    "    if stats[word]['annt_freq'] > stats[word]['text_freq'] :\n",
    "      print(\"%20s >>> text-freq = %.5f  quote-freq = \\x1b[31m%.5f\\x1b[0m\"%(word, stats[word]['text_freq'] , stats[word]['annt_freq']))\n",
    "    else:\n",
    "      print(\"%20s >>> text-freq = %.5f  quote-freq = %.5f\"%(word, stats[word]['text_freq'] , stats[word]['annt_freq']))\n",
    "      \n",
    "  print('\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         quote\n",
      "0     lazy dog\n",
      "1  jumped over\n",
      "2    brown fox\n",
      "the quick brown fox jumped over the lazy dog more dog rhubarb rhubarb dog rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarbrhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb\n",
      "text dictionary\n",
      "Counter({('rhubarb', 'rhubarb'): 49, ('dog', 'rhubarb'): 2, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumped'): 1, ('jumped', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', 'more'): 1, ('more', 'dog'): 1, ('rhubarb', 'dog'): 1, ('rhubarb', 'rhubarbrhubarb'): 1, ('rhubarbrhubarb', 'rhubarb'): 1})\n",
      "quote region [(10, 19), (20, 31), (36, 44)]\n",
      "{'more dog': {'annt_count': 0, 'text_count': 1, 'weight': 0.4}, 'lazy fox': {'annt_count': 0, 'text_count': 0, 'weight': 0.2}, 'brown fox': {'annt_count': 1, 'text_count': 1, 'weight': 0.3}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#  #  #  # #  #  # #  #  # #  #  # #  #  # #  #  #\n",
    "#   #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #\n",
    "# test code for get_ngram_score()\n",
    "\n",
    "# create simple panda data frame\n",
    "col_names =  ['quote']\n",
    "quotes_df  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "quotes_df.loc[0] = [\"lazy dog\"]\n",
    "quotes_df.loc[1] = [\"jumped over\"]\n",
    "quotes_df.loc[2] = [\"brown fox\"]\n",
    "print(quotes_df)\n",
    "\n",
    "text = 'the quick brown fox jumped over the lazy dog more dog rhubarb rhubarb dog rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarbrhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb'\n",
    "print(text)\n",
    "n = 2 \n",
    "text_dict = Counter(ngrams(re.split(r'%s|$' % r'\\W+' , text),n))\n",
    "print(\"text dictionary\")\n",
    "print(text_dict)\n",
    "\n",
    "words_list = [((\"more\" ,\"dog\"),0.4),((\"lazy\",\"fox\"),0.2),((\"brown\",\"fox\"),0.3)]\n",
    "\n",
    "quotes_list = [quote for quote in quotes_df['quote']]\n",
    "test_quote_idxs = get_quote_idxs(text,quotes_list)\n",
    "\n",
    "quote_region = build_quoted_regions(test_quote_idxs)\n",
    "print(\"quote region\", quote_region)\n",
    "my_score = get_ngram_scores(words_list, text, text_dict, quote_region)\n",
    "print(my_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': {'annt_count': 1, 'text_count': 3, 'weight': 0.4},\n",
       " 'rhubarb': {'annt_count': 0, 'text_count': 52, 'weight': 0.2}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         quote\n",
      "0     lazy dog\n",
      "1  jumped over\n",
      "text dictionary\n",
      "Counter({'rhubarb': 52, 'dog': 3, 'the': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumped': 1, 'over': 1, 'lazy': 1, 'more': 1, 'rhubarbrhubarb': 1})\n",
      "quote region [(20, 31), (36, 44)]\n",
      "{'dog': {'annt_count': 1, 'text_count': 3, 'weight': 0.4}, 'rhubarb': {'annt_count': 0, 'text_count': 52, 'weight': 0.2}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# test code \n",
    "\n",
    "col_names =  ['quote']\n",
    "quotes_df  = pd.DataFrame(columns = col_names)\n",
    "# my_df\n",
    "# If you want to add a record to the dataframe it would be better to use:\n",
    "\n",
    "# quotes_df.loc[len(my_df)] = [\"lazy dog\"]\n",
    "quotes_df.loc[0] = [\"lazy dog\"]\n",
    "quotes_df.loc[1] = [\"jumped over\"]\n",
    "# quotes_df.loc[1] = [\"quick fox\"]\n",
    "print(quotes_df)\n",
    "\n",
    "text = 'the quick brown fox jumped over the lazy dog more dog rhubarb rhubarb dog rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarbrhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb'\n",
    "text_dict = Counter(np.array(re.split(r'%s|$' % r'\\W+' , text)))\n",
    "print(\"text dictionary\")\n",
    "print(text_dict)\n",
    "\n",
    "words_list = [(\"dog\",0.4),(\"rhubarb\",0.2)]\n",
    "\n",
    "quotes_list = [quote for quote in quotes_df['quote']]\n",
    "test_quote_idxs = get_quote_idxs(text,quotes_list)\n",
    "\n",
    "\n",
    "quote_region = build_quoted_regions(test_quote_idxs)\n",
    "print(\"quote region\", quote_region)\n",
    "my_score = get_word_scores(words_list, text, text_dict, quote_region)\n",
    "print(my_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sorted list of individual page__id's\n",
    "page_id_set = set(testing['page__id'])\n",
    "page_id_list = list(page_id_set)\n",
    "page_id_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  133\n",
      "page id =  134\n",
      "page id =  135\n",
      "page id =  136\n",
      "page id =  137\n",
      "page id =  138\n",
      "page id =  139\n",
      "page id =  140\n",
      "page id =  141\n",
      "page id =  142\n",
      "page id =  143\n",
      "page id =  144\n",
      "page id =  145\n",
      "page id =  146\n",
      "page id =  147\n",
      "page id =  148\n",
      "page id =  149\n",
      "page id =  150\n",
      "page id =  151\n",
      "page id =  152\n",
      "page id =  153\n",
      "page id =  154\n",
      "page id =  155\n",
      "page id =  156\n",
      "page id =  157\n",
      "page id =  158\n",
      "page id =  159\n",
      "page id =  160\n",
      "page id =  161\n",
      "page id =  162\n",
      "page id =  163\n",
      "page id =  164\n",
      "page id =  165\n",
      "page id =  166\n",
      "page id =  167\n",
      "page id =  168\n",
      "page id =  169\n",
      "page id =  170\n",
      "page id =  171\n",
      "page id =  172\n",
      "page id =  173\n",
      "page id =  174\n",
      "page id =  175\n",
      "page id =  181\n",
      "page id =  182\n",
      "page id =  183\n",
      "page id =  184\n",
      "page id =  185\n",
      "page id =  186\n",
      "page id =  187\n",
      "page id =  188\n",
      "page id =  189\n",
      "page id =  190\n",
      "page id =  191\n",
      "page id =  192\n",
      "page id =  193\n",
      "page id =  194\n",
      "page id =  195\n",
      "page id =  196\n",
      "page id =  197\n",
      "page id =  198\n",
      "page id =  199\n",
      "page id =  200\n",
      "page id =  201\n",
      "page id =  202\n",
      "page id =  203\n",
      "page id =  204\n",
      "page id =  205\n",
      "page id =  206\n",
      "page id =  207\n",
      "page id =  208\n",
      "page id =  210\n",
      "page id =  211\n",
      "page id =  212\n",
      "page id =  213\n",
      "page id =  214\n",
      "page id =  216\n",
      "page id =  218\n",
      "page id =  219\n",
      "page id =  220\n",
      "page id =  221\n",
      "page id =  222\n",
      "page id =  223\n",
      "page id =  224\n",
      "page id =  225\n",
      "page id =  226\n",
      "page id =  228\n",
      "page id =  229\n",
      "page id =  230\n",
      "page id =  231\n",
      "page id =  232\n",
      "page id =  233\n",
      "page id =  234\n",
      "page id =  235\n",
      "page id =  236\n",
      "page id =  237\n",
      "page id =  238\n",
      "page id =  239\n",
      "page id =  240\n",
      "page id =  241\n",
      "page id =  242\n",
      "page id =  243\n",
      "page id =  244\n",
      "page id =  245\n",
      "page id =  246\n",
      "page id =  247\n",
      "page id =  248\n",
      "page id =  250\n",
      "page id =  251\n",
      "page id =  252\n",
      "page id =  253\n",
      "page id =  254\n",
      "page id =  255\n",
      "page id =  256\n",
      "page id =  257\n",
      "page id =  258\n",
      "page id =  259\n",
      "page id =  260\n",
      "page id =  261\n",
      "page id =  262\n",
      "page id =  263\n",
      "page id =  264\n",
      "page id =  265\n",
      "page id =  266\n",
      "page id =  267\n",
      "page id =  279\n",
      "page id =  281\n",
      "page id =  283\n",
      "page id =  284\n",
      "page id =  285\n",
      "page id =  286\n",
      "page id =  287\n",
      "page id =  288\n",
      "page id =  289\n",
      "page id =  290\n",
      "page id =  291\n",
      "page id =  292\n",
      "page id =  293\n",
      "page id =  294\n",
      "page id =  295\n",
      "page id =  296\n",
      "page id =  297\n",
      "page id =  298\n",
      "page id =  299\n",
      "page id =  300\n",
      "page id =  301\n",
      "page id =  302\n",
      "page id =  303\n",
      "page id =  304\n",
      "page id =  305\n",
      "page id =  306\n"
     ]
    }
   ],
   "source": [
    "# LIME parameters\n",
    "num_features = 10\n",
    "\n",
    "# experiment parameters\n",
    "debug_display = False\n",
    "MIN_TXT_SIZE = 500\n",
    "rand_control_num = 500\n",
    "num_samples = 5000 # number of samples LIME uses to build it's model\n",
    "stop_words = True # True - stop words are used ; False - stop words are removed  \n",
    "\n",
    "num_articles = len(page_id_list)\n",
    "\n",
    "def open_results_files(ts):\n",
    "  ts = time.localtime()\n",
    "  results_file = \"results_\" + time.strftime(\"%H%M%S_%d%m%Y.json\", ts)\n",
    "  errors_file = \"errors_\" + time.strftime(\"%H%M%S_%d%m%Y.txt\", ts)\n",
    "  try:\n",
    "    fr = open(results_file,'w')\n",
    "  except:\n",
    "    print(\"error opening file %s\" % (results_file))\n",
    "  try:\n",
    "    fe = open(errors_file,'w')\n",
    "  except:\n",
    "    print(\"error opening file %s\" % (errors_file)) \n",
    "  return fr, fe \n",
    "\n",
    "# def run_experiment(num_articles, num_features,num_samples)\n",
    "# ********** MAIN LOOP *************\n",
    "# look at first [num_articles] pages\n",
    "all_word_scores = []\n",
    "all_random_word_scores = []\n",
    "\n",
    "ts = time.localtime()\n",
    "timestamp =  time.strftime(\"%H%M%S_%d%m%Y\", ts)\n",
    "\n",
    "desc_string = \"500 sets of random words, added word weight from LIME\"\n",
    "short_desc = \"Bigram v002\"\n",
    "num_trials = 1\n",
    "\n",
    "\n",
    "#  note - JSON keys have to be strings (unlike python dict keys)\n",
    "experiment_dict = {'timestamp':timestamp,\n",
    "                   'description': desc_string ,\n",
    "                   'short_desc' : short_desc,\n",
    "                     'trials':\n",
    "                       {str(trial):\n",
    "                         {\n",
    "                           'num_articles':num_articles,\n",
    "                           'num_features':num_features,\n",
    "                           'num_samples':num_samples,\n",
    "                           'rand_sample_size':rand_control_num,\n",
    "                           'stop_words': stop_words,\n",
    "                           'corpus':test_file,\n",
    "                           'run_time':'',\n",
    "                           'articles':\n",
    "                             {str(page_id):\n",
    "                               {\n",
    "                               'score':0.0,\n",
    "                               'txt_len':0,\n",
    "                               'quote_len':0,\n",
    "                               'explainer_words':[],\n",
    "                               'random_words':[]\n",
    "                               } for page_id in page_id_list[:num_articles]                    \n",
    "                             }\n",
    "                         }for trial in range(num_trials)\n",
    "                       }\n",
    "                  }\n",
    "                    \n",
    "\n",
    "res_file, err_file = open_results_files(ts)\n",
    "\n",
    "\n",
    "page_dict = {}\n",
    "# how many times to select random word comparision set from same text \n",
    "\n",
    "trial_key = str(0)\n",
    "# for page_id in page_id_list[:num_articles]:\n",
    "# for page_id in page_id_list:\n",
    "###################################################\n",
    "## LOOP THROUGH ARTICLES                         ##\n",
    "###################################################\n",
    "for page_id in page_id_list:\n",
    "# for page_id in page_id_list[:2]:\n",
    "  page_key = str(page_id)\n",
    "  print(\"page id = \",page_id)\n",
    "#  get text of article\n",
    "  text = testing[testing['page__id'] == page_id].iloc[0]['text']\n",
    "  text = text.lower()\n",
    "\n",
    "#   skip to next article, if article text is too small\n",
    "  if len(text) < MIN_TXT_SIZE:\n",
    "    continue\n",
    "\n",
    "  #   Run LIME explainer to get explainer words\n",
    "  exp = explainer.explain_instance(text, pred_proba_func, num_features=num_features, num_samples=num_samples)\n",
    "  if debug_display:\n",
    "    print('Probability(biased) =', c.predict_proba([text])[0,0])\n",
    "  experiment_dict['trials'][trial_key]['articles'][page_key]['score'] = exp.score\n",
    "  exp_words = [word for (word,weight) in exp.as_list()]\n",
    "\n",
    "  # find all quotes for given article\n",
    "  quotes_list_df = get_quotes_list(testing, page_id)\n",
    "  quotes_list = [quote.lower() for quote in quotes_list_df['quote']]\n",
    "  quotes_idxs = get_quote_idxs(text, quotes_list, err_file)\n",
    "  quote_regions = build_quoted_regions(quotes_idxs)\n",
    "  \n",
    "  text_as_list = re.split(r'%s|$' % r'\\W+' , text)\n",
    "  n=2\n",
    "  text_as_ngrams = ([ngram for ngram in (ngrams(text_as_list,n))])\n",
    "\n",
    "  text_count_dict = Counter(text_as_ngrams) \n",
    "  text_as_ngrams = np.array(text_as_ngrams)\n",
    "                            \n",
    "# don't filter out stop words\n",
    "  \n",
    "  #   LIME WORDS\n",
    "  #   ngram_scores = get_ngram_scores(exp.as_list(), text, text_count_dict, quote_regions )\n",
    "  ngram_scores = get_ngram_scores([(tuple(ngram_tuple[0].split() ),ngram_tuple[1]) for ngram_tuple in exp.as_list()], text, text_count_dict, quote_regions )\n",
    "  experiment_dict['trials'][trial_key]['articles'][page_key]['explainer_words'] = ngram_scores\n",
    "\n",
    "  #   RANDOM WORDS\n",
    "  #   control group of bigrams randomly selected from article    \n",
    "  for i in range(rand_control_num):\n",
    "    all_random_word_scores = []\n",
    "#     rand_choice = np.random.choice(text_as_set_list.size, num_features, replace=False)\n",
    "#     random_words = [(random,0.0) for random in text_as_set_list[rand_choice]]    \n",
    "    rand_choice = np.random.choice(text_as_ngrams.shape[0], num_features, replace=False)\n",
    "#     ngrams are 2 dim arrays, use them to create tuples\n",
    "    random_ngrams = [((random[0],random[1]),0.0) for random in text_as_ngrams[rand_choice]]\n",
    "    if debug_display:\n",
    "      print(random_ngrams)\n",
    "    random_word_scores = get_ngram_scores(random_ngrams, text, text_count_dict, quote_regions)\n",
    "    experiment_dict['trials'][trial_key]['articles'][page_key]['random_words'].append(random_word_scores)\n",
    "\n",
    "# if 229 in experiment_dict['trials']['0']['articles']:\n",
    "#   print(\"key 229 present\")\n",
    "json.dump(experiment_dict,res_file)\n",
    "res_file.close()\n",
    "err_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
