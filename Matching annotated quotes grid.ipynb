{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03-JUL-18\n",
    "\n",
    "Adaptation of \"LIME - basic LR\"\n",
    "\n",
    "Trains on the hyperpartisan data\n",
    "\n",
    "Tests on annotated data from Briefr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/rick/factmata/factnlp-experimental/lime')\n",
    "\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/rick/factmata/factmata-quality-engine/factnlp')\n",
    "sys.path.append('/Users/rick/factmata/utils')\n",
    "sys.path.append('/Users/rick/factmata/fastText')\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factnlp.category.category_predictor import CategoryPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-25 21:26:17,217 | INFO | tasks.py-0080 | Loading models\n"
     ]
    }
   ],
   "source": [
    "import settings\n",
    "import distributor.tasks\n",
    "models = distributor.tasks.models_loader_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Google drive paths\n",
    "\n",
    "labelled_data_path = '/hyperpartisanship/CF labelled data'\n",
    "training_data_path = '/Users/mariarmestre/Projects/factnlp/models/hyperpartisanship/current/'\n",
    "\n",
    "output_data_path = '/hyperpartisanship/error analysis/with CF labelled data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-25 21:26:17,237 | INFO | modelsloader.py-0068 | Loading Politics Classifier Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-25 21:27:32,869 | INFO | modelsloader.py-0070 | Loading Hyperpartisanship Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator Pipeline from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<factnlp.hyperpartisanship.hyperpartisanship_predictor.HyperpartisanshipPredictor at 0x12434c7b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Production model\n",
    "\n",
    "production_hp = models.get_hyperpartisanship_model()\n",
    "production_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function to provide predict_proba() for LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ad hoc func to serve as predict_proba() for LIME\n",
    "import json\n",
    "classes = ['Biased','Unbiased']\n",
    "\n",
    "def fm_predict_proba(in_text_list,verbose=False):\n",
    "  \"\"\"\n",
    "  Takes a list of texts and for each gets a probability of being hyperpartisan from the hyperpartisan model\n",
    "  Then extends the probability returned into a tuple of (probability of true, probability of false) as required\n",
    "  by the LIME explain.explainer_instance function\n",
    "  \n",
    "  Args:\n",
    "    list of texts represented as strings - prediction will be run against each \n",
    "  \n",
    "  Returns:\n",
    "    numpy array of tuples - each represents (p(is hyperpartisan),p(is not hyperpartisan))\n",
    "  \"\"\"\n",
    "  \n",
    "  probs = []\n",
    "  if verbose:\n",
    "    print(\"base text length =\", len(in_text_list[0]))\n",
    "    print(\"fm_predict_proba - number of elements in text list \", len(in_text_list))\n",
    "  text_list = in_text_list\n",
    "  prob1 = prob2 = 0 \n",
    "  t0 = time.time()\n",
    "#   call to hyperpartisan classifier\n",
    "  results = production_hp.predict(text_list)\n",
    "  t1 = time.time()\n",
    "\n",
    "  if verbose:\n",
    "    print(\"predict run time = \",str(datetime.timedelta(seconds=round(t1-t0,0))))\n",
    "  for result in results :  \n",
    "    if result['class'] == classes[0]:\n",
    "      prob1 = result['score']\n",
    "      prob2 = 1 - prob1\n",
    "    else: \n",
    "      prob2 = result['score']\n",
    "      prob1 = 1 - prob2\n",
    "    probs.append([prob1,prob2])\n",
    "    \n",
    "  return(np.array(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00235569, 0.99764431]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test if needed\n",
    "test_txt1 = []\n",
    "test_txt1.append(\"President Donald Trump was in the Philippines on Monday as part of the final stop on a whirlwind, 12-day tour of Asia that included warm receptions by the gracious hosts of Japan, South Korea, China and Vietnam, according to Fox News.\")\n",
    "res = fm_predict_proba(test_txt1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data, train a simple LR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "test_file = \"/Users/rick/factmata/article_quotes.csv\"\n",
    "\n",
    "# training = pd.read_csv(\"/Users/rick/factmata/train.csv\")\n",
    "training = pd.read_csv(\"/Users/rick/factmata/factmata-quality-engine/factnlp/models/hyperpartisanship/current/train.csv\")\n",
    "\n",
    "testing = pd.read_csv(test_file)\n",
    "class_names = ['Biased','Unbiased']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(training['text'])\n",
    "train_targets = training['tag']\n",
    "test_vectors = vectorizer.transform(testing['text'])\n",
    "test_targets = testing['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(C=1e5)\n",
    "lr.fit(train_vectors,train_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizer, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an explainer object \n",
    "Select text and run explainer\n",
    "Create list of explainer words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# s = testing['id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose which model to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_MODEL = 0\n",
    "FM_MODEL = 1 \n",
    "\n",
    "# model_used = LR_MODEL\n",
    "model_used = FM_MODEL\n",
    "\n",
    "if model_used == FM_MODEL : \n",
    "  pred_proba_func = fm_predict_proba\n",
    "  num_samples = 1000\n",
    "else :\n",
    "  pred_proba_func = c.predict_proba\n",
    "  num_samples = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  function to return all rows (i.e. annotations) matching a page_id\n",
    "def get_quotes_list(testing, page_id):\n",
    "  \"\"\"\n",
    "   Args:\n",
    "     testing: panda data frame with text, quotes etc\n",
    "     page_id: id of piece of text (as represented in 'page__id' of data frame)\n",
    "   \n",
    "   Returns:\n",
    "     List of all rows in data frame that match page_id (i.e. all rows which represent\n",
    "     quotes from this piece of text)\n",
    "  \"\"\"\n",
    "\n",
    "#   TODO improve error handling ?\n",
    "#   if page_id not in testing['page__id']:\n",
    "#     return [] \n",
    "  page_refs = (testing['page__id'] == page_id)\n",
    "  pages_list = testing[page_refs]\n",
    "  return pages_list\n",
    "\n",
    "def get_word_scores(word_list, text, text_as_dict, quotes_region): \n",
    "  \"\"\"\n",
    "  Produces score on how often the words appear in the text and in the quotes region\n",
    "  (Simpler version of get_word_stats, which uses a list of quotes rather than unified\n",
    "  quote regions)\n",
    "\n",
    "  Args:\n",
    "    words: a subset of words or ngrams from the text \n",
    "    text : the text itself\n",
    "    text_as_dict:  dict with key as a word and value its occurence count e.g {'fall':3}\n",
    "    quotes_region : list of tuples in form (start, stop) indicating position of quote regions in text\n",
    "\n",
    "  Returns: Dict - key is word, value is dict with annt_count - no of occurences of word in quote region\n",
    "                                                  text_count - no of occurences of word in entire text \n",
    "                                \n",
    "  \"\"\"\n",
    "  \n",
    "  debug_list = [(start,stop) for (start,stop) in quotes_region]\n",
    "#   print(\"quotes region = \", sorted(quotes_region))\n",
    "  all_quotes = ' '.join([text[start:stop] for (start,stop) in quotes_region])\n",
    "  all_quotes_count_dict = Counter(np.array(re.split(r'%s|$' % r'\\W+' , all_quotes)))\n",
    "  scores = {word:{'annt_count':0,\n",
    "                 'text_count':0,\n",
    "                  'weight':weight\n",
    "                } for (word,weight) in word_list}\n",
    "  for word,_ in word_list:  \n",
    "    scores[word]['text_count'] = text_as_dict[word]\n",
    "    scores[word]['annt_count'] = all_quotes_count_dict[word]    \n",
    "  return scores\n",
    "\n",
    "def display_stats(exp_words, stats):\n",
    "  for word in exp_words:\n",
    "    if stats[word]['annt_count'] != 0 :\n",
    "      word_stat = stats[word]\n",
    "      idf = word_stat['idf']\n",
    "    if stats[word]['annt_freq'] > stats[word]['text_freq'] :\n",
    "      print(\"%20s >>> text-freq = %.5f  quote-freq = \\x1b[31m%.5f\\x1b[0m\"%(word, stats[word]['text_freq'] , stats[word]['annt_freq']))\n",
    "    else:\n",
    "      print(\"%20s >>> text-freq = %.5f  quote-freq = %.5f\"%(word, stats[word]['text_freq'] , stats[word]['annt_freq']))\n",
    "      \n",
    "  print('\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and index utility functions - part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_quoted_regions(quoted_parts,randomize = False):\n",
    "    \"\"\"\n",
    "    sort quoted parts, then merge any which overlap or form\n",
    "    continuous quotes\n",
    "\n",
    "    Args :\n",
    "        list of tuples representing sections of text\n",
    "    Returns:\n",
    "        The same but with sections merged to continuous regions where\n",
    "        appropriate\n",
    "    \"\"\"\n",
    "    def purge_list(target_list, remove_list):\n",
    "        for i in reversed(remove_list):\n",
    "            del(target_list[i])\n",
    "        return target_list\n",
    "\n",
    "    remove_list = []\n",
    "    ####  remove any dodgy tuples ####\n",
    "    # check for tuples with start point after stop point, or negative start\n",
    "    for i in range(len(quoted_parts)):\n",
    "        start,stop = quoted_parts[i]\n",
    "        if (start < 0) or (start > stop):\n",
    "            print(\"warning - start point should be non-negative and not greater than stop point in \", quoted_parts[i])\n",
    "            remove_list.append(i)\n",
    "    # for i in reversed(remove_list):\n",
    "    #     del(quoted_parts[i])\n",
    "    purge_list(quoted_parts,remove_list)\n",
    "\n",
    "    #### main sort and merge ####\n",
    "    quoted_parts = sorted(quoted_parts)\n",
    "    quoted_parts_len = len(quoted_parts)\n",
    "    remove_list = []\n",
    "    for i in range(quoted_parts_len - 1):\n",
    "        # compare end of one part with start of other - merge if overlap\n",
    "        start1, stop1 = quoted_parts[i]\n",
    "        start2, stop2 = quoted_parts[i + 1]\n",
    "        if stop1 >= start2:\n",
    "            if stop2 <= stop1:\n",
    "                # remove subsumed part\n",
    "                remove_list.append(i+1)\n",
    "            else:\n",
    "                # second part absorbs first, first is removed\n",
    "                quoted_parts[i + 1] = (start1, stop2)\n",
    "                remove_list.append(i)\n",
    "    purge_list(quoted_parts,remove_list)\n",
    "\n",
    "    return quoted_parts\n",
    "  \n",
    "def randomize_regions(quoted_parts,text_len):\n",
    "  \"\"\"\n",
    "  Takes some quoted regions and returns randomized control regions of \n",
    "  the same length and number\n",
    "  \"\"\"\n",
    "  num_regions = len(quoted_parts)\n",
    "  num_rand_regions = 0 \n",
    "  rand_regions = []\n",
    "  attempts = 0\n",
    "  while (num_rand_regions < num_regions) and (attempts < 100):\n",
    "    start = random.randint(0,text_len-1)\n",
    "    (quote_start,quote_stop) = quoted_parts[num_rand_regions]\n",
    "    region_len = quote_stop - quote_start \n",
    "    stop = start + region_len \n",
    "    print(start,stop)\n",
    "    if  stop > text_len - 1 :\n",
    "      attempts += 1 \n",
    "      continue\n",
    "    else:\n",
    "      overlap = False\n",
    "      for i in range(num_rand_regions):\n",
    "        if (start in rand_regions[i]) or (stop in rand_regions[i]):\n",
    "          overlap = True\n",
    "          continue\n",
    "#   if there's an overlap don't keep this region , find a new one \n",
    "      if overlap:\n",
    "        continue\n",
    "      else:\n",
    "        rand_regions.append((start,stop))\n",
    "        num_rand_regions += 1\n",
    "  return rand_regions\n",
    "        \n",
    "##############################\n",
    "\n",
    "def get_quote_idxs(txt, quotes, err_file = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        txt - string\n",
    "        quotes - list of string\n",
    "    Returns:\n",
    "        list of tuples indicating (start,stop) position of each quote in txt (if doesn't appear it's ignored)\n",
    "    \"\"\"\n",
    "    quoted_idxs = []\n",
    "    for quote in quotes:\n",
    "      start = txt.find(quote)\n",
    "      if start != -1 :\n",
    "        quoted_idxs.append((start,start+len(quote)))\n",
    "      else:\n",
    "        if err_file is not None:\n",
    "          err_file.write(\"\\n*START OF TEXT*\\n\")\n",
    "          err_file.write(txt)\n",
    "          err_file.write(\"\\n\")\n",
    "          err_file.write(\"** END OF TEXT*\\n\")\n",
    "          err_file.write(\"quote = \")\n",
    "          err_file.write(quote)\n",
    "          err_file.write(\"\\n***\\n\")\n",
    "        \n",
    "    return quoted_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         quote\n",
      "0     lazy dog\n",
      "1  jumped over\n",
      "text dictionary\n",
      "Counter({'rhubarb': 52, 'dog': 3, 'the': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumped': 1, 'over': 1, 'lazy': 1, 'more': 1, 'rhubarbrhubarb': 1})\n",
      "quote region [(20, 31), (36, 44)]\n",
      "{'dog': {'annt_count': 1, 'text_count': 3, 'weight': 0.4}, 'rhubarb': {'annt_count': 0, 'text_count': 52, 'weight': 0.2}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# test code \n",
    "\n",
    "col_names =  ['quote']\n",
    "quotes_df  = pd.DataFrame(columns = col_names)\n",
    "# my_df\n",
    "# If you want to add a record to the dataframe it would be better to use:\n",
    "\n",
    "# quotes_df.loc[len(my_df)] = [\"lazy dog\"]\n",
    "quotes_df.loc[0] = [\"lazy dog\"]\n",
    "quotes_df.loc[1] = [\"jumped over\"]\n",
    "# quotes_df.loc[1] = [\"quick fox\"]\n",
    "print(quotes_df)\n",
    "\n",
    "text = 'the quick brown fox jumped over the lazy dog more dog rhubarb rhubarb dog rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarbrhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb rhubarb'\n",
    "text_dict = Counter(np.array(re.split(r'%s|$' % r'\\W+' , text)))\n",
    "print(\"text dictionary\")\n",
    "print(text_dict)\n",
    "\n",
    "words_list = [(\"dog\",0.4),(\"rhubarb\",0.2)]\n",
    "\n",
    "quotes_list = [quote for quote in quotes_df['quote']]\n",
    "test_quote_idxs = get_quote_idxs(text,quotes_list)\n",
    "\n",
    "\n",
    "quote_region = build_quoted_regions(test_quote_idxs)\n",
    "print(\"quote region\", quote_region)\n",
    "my_score = get_word_scores(words_list, text, text_dict, quote_region)\n",
    "print(my_score)\n",
    "\n",
    "\n",
    "# display_stats(words,my_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/Users/rick/factmata/factnlp-experimental/lime/lime/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lime\n",
    "print(lime.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sorted list of individual page__id's\n",
    "page_id_set = set(testing['page__id'])\n",
    "page_id_list = list(page_id_set)\n",
    "page_id_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  132 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  133 1\n",
      "page id =  134 1\n",
      "page id =  135 1\n",
      "page id =  136 1\n",
      "page id =  137 1\n",
      "page id =  138 1\n",
      "page id =  139 1\n",
      "page id =  140 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number8.276122e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  141 1\n",
      "page id =  142 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number8.195989e-21\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  143 1\n",
      "page id =  144 1\n",
      "page id =  145 1\n",
      "page id =  146 1\n",
      "page id =  147 1\n",
      "page id =  148 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:156: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  dual_coef = linalg.lstsq(K, y)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  149 1\n",
      "page id =  150 1\n",
      "page id =  151 1\n",
      "page id =  152 1\n",
      "page id =  153 1\n",
      "page id =  154 1\n",
      "page id =  155 1\n",
      "page id =  156 1\n",
      "page id =  157 1\n",
      "page id =  158 1\n",
      "page id =  159 1\n",
      "page id =  160 1\n",
      "page id =  161 1\n",
      "page id =  162 1\n",
      "page id =  163 1\n",
      "page id =  164 1\n",
      "page id =  165 1\n",
      "page id =  166 1\n",
      "page id =  167 1\n",
      "page id =  168 1\n",
      "page id =  169 1\n",
      "page id =  170 1\n",
      "page id =  171 1\n",
      "page id =  172 1\n",
      "page id =  173 1\n",
      "page id =  174 1\n",
      "page id =  175 1\n",
      "page id =  181 1\n",
      "page id =  182 1\n",
      "page id =  183 1\n",
      "page id =  184 1\n",
      "page id =  185 1\n",
      "page id =  186 1\n",
      "page id =  187 1\n",
      "page id =  188 1\n",
      "page id =  189 1\n",
      "page id =  190 1\n",
      "page id =  191 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number8.123514e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  192 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number2.959118e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  193 1\n",
      "page id =  194 1\n",
      "page id =  195 1\n",
      "page id =  196 1\n",
      "page id =  197 1\n",
      "page id =  198 1\n",
      "page id =  199 1\n",
      "page id =  200 1\n",
      "page id =  201 1\n",
      "page id =  202 1\n",
      "page id =  203 1\n",
      "page id =  204 1\n",
      "page id =  205 1\n",
      "page id =  206 1\n",
      "page id =  207 1\n",
      "page id =  208 1\n",
      "page id =  210 1\n",
      "page id =  211 1\n",
      "page id =  212 1\n",
      "page id =  213 1\n",
      "page id =  214 1\n",
      "page id =  216 1\n",
      "page id =  218 1\n",
      "page id =  219 1\n",
      "page id =  220 1\n",
      "page id =  221 1\n",
      "page id =  222 1\n",
      "page id =  223 1\n",
      "page id =  224 1\n",
      "page id =  225 1\n",
      "page id =  226 1\n",
      "page id =  228 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number1.801517e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  229 1\n",
      "page id =  230 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number1.520251e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  231 1\n",
      "page id =  232 1\n",
      "page id =  233 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number5.831598e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  234 1\n",
      "page id =  235 1\n",
      "page id =  236 1\n",
      "page id =  237 1\n",
      "page id =  238 1\n",
      "page id =  239 1\n",
      "page id =  240 1\n",
      "page id =  241 1\n",
      "page id =  242 1\n",
      "page id =  243 1\n",
      "page id =  244 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number7.553052e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  245 1\n",
      "page id =  246 1\n",
      "page id =  247 1\n",
      "page id =  248 1\n",
      "page id =  250 1\n",
      "page id =  251 1\n",
      "page id =  252 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number2.556344e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  253 1\n",
      "page id =  254 1\n",
      "page id =  255 1\n",
      "page id =  256 1\n",
      "page id =  257 1\n",
      "page id =  258 1\n",
      "page id =  259 1\n",
      "page id =  260 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number3.117197e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  261 1\n",
      "page id =  262 1\n",
      "page id =  263 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number3.610517e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  264 1\n",
      "page id =  265 1\n",
      "page id =  266 1\n",
      "page id =  267 1\n",
      "page id =  279 1\n",
      "page id =  281 1\n",
      "page id =  283 1\n",
      "page id =  284 1\n",
      "page id =  285 1\n",
      "page id =  286 1\n",
      "page id =  287 1\n",
      "page id =  288 1\n",
      "page id =  289 1\n",
      "page id =  290 1\n",
      "page id =  291 1\n",
      "page id =  292 1\n",
      "page id =  293 1\n",
      "page id =  294 1\n",
      "page id =  295 1\n",
      "page id =  296 1\n",
      "page id =  297 1\n",
      "page id =  298 1\n",
      "page id =  299 1\n",
      "page id =  300 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number8.528280e-20\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  301 1\n",
      "page id =  302 1\n",
      "page id =  303 1\n",
      "page id =  304 1\n",
      "page id =  305 1\n",
      "page id =  306 1\n",
      "page id =  132 2\n",
      "page id =  133 2\n",
      "page id =  134 2\n",
      "page id =  135 2\n",
      "page id =  136 2\n",
      "page id =  137 2\n",
      "page id =  138 2\n",
      "page id =  139 2\n",
      "page id =  140 2\n",
      "page id =  141 2\n",
      "page id =  142 2\n",
      "page id =  143 2\n",
      "page id =  144 2\n",
      "page id =  145 2\n",
      "page id =  146 2\n",
      "page id =  147 2\n",
      "page id =  148 2\n",
      "page id =  149 2\n",
      "page id =  150 2\n",
      "page id =  151 2\n",
      "page id =  152 2\n",
      "page id =  153 2\n",
      "page id =  154 2\n",
      "page id =  155 2\n",
      "page id =  156 2\n",
      "page id =  157 2\n",
      "page id =  158 2\n",
      "page id =  159 2\n",
      "page id =  160 2\n",
      "page id =  161 2\n",
      "page id =  162 2\n",
      "page id =  163 2\n",
      "page id =  164 2\n",
      "page id =  165 2\n",
      "page id =  166 2\n",
      "page id =  167 2\n",
      "page id =  168 2\n",
      "page id =  169 2\n",
      "page id =  170 2\n",
      "page id =  171 2\n",
      "page id =  172 2\n",
      "page id =  173 2\n",
      "page id =  174 2\n",
      "page id =  175 2\n",
      "page id =  181 2\n",
      "page id =  182 2\n",
      "page id =  183 2\n",
      "page id =  184 2\n",
      "page id =  185 2\n",
      "page id =  186 2\n",
      "page id =  187 2\n",
      "page id =  188 2\n",
      "page id =  189 2\n",
      "page id =  190 2\n",
      "page id =  191 2\n",
      "page id =  192 2\n",
      "page id =  193 2\n",
      "page id =  194 2\n",
      "page id =  195 2\n",
      "page id =  196 2\n",
      "page id =  197 2\n",
      "page id =  198 2\n",
      "page id =  199 2\n",
      "page id =  200 2\n",
      "page id =  201 2\n",
      "page id =  202 2\n",
      "page id =  203 2\n",
      "page id =  204 2\n",
      "page id =  205 2\n",
      "page id =  206 2\n",
      "page id =  207 2\n",
      "page id =  208 2\n",
      "page id =  210 2\n",
      "page id =  211 2\n",
      "page id =  212 2\n",
      "page id =  213 2\n",
      "page id =  214 2\n",
      "page id =  216 2\n",
      "page id =  218 2\n",
      "page id =  219 2\n",
      "page id =  220 2\n",
      "page id =  221 2\n",
      "page id =  222 2\n",
      "page id =  223 2\n",
      "page id =  224 2\n",
      "page id =  225 2\n",
      "page id =  226 2\n",
      "page id =  228 2\n",
      "page id =  229 2\n",
      "page id =  230 2\n",
      "page id =  231 2\n",
      "page id =  232 2\n",
      "page id =  233 2\n",
      "page id =  234 2\n",
      "page id =  235 2\n",
      "page id =  236 2\n",
      "page id =  237 2\n",
      "page id =  238 2\n",
      "page id =  239 2\n",
      "page id =  240 2\n",
      "page id =  241 2\n",
      "page id =  242 2\n",
      "page id =  243 2\n",
      "page id =  244 2\n",
      "page id =  245 2\n",
      "page id =  246 2\n",
      "page id =  247 2\n",
      "page id =  248 2\n",
      "page id =  250 2\n",
      "page id =  251 2\n",
      "page id =  252 2\n",
      "page id =  253 2\n",
      "page id =  254 2\n",
      "page id =  255 2\n",
      "page id =  256 2\n",
      "page id =  257 2\n",
      "page id =  258 2\n",
      "page id =  259 2\n",
      "page id =  260 2\n",
      "page id =  261 2\n",
      "page id =  262 2\n",
      "page id =  263 2\n",
      "page id =  264 2\n",
      "page id =  265 2\n",
      "page id =  266 2\n",
      "page id =  267 2\n",
      "page id =  279 2\n",
      "page id =  281 2\n",
      "page id =  283 2\n",
      "page id =  284 2\n",
      "page id =  285 2\n",
      "page id =  286 2\n",
      "page id =  287 2\n",
      "page id =  288 2\n",
      "page id =  289 2\n",
      "page id =  290 2\n",
      "page id =  291 2\n",
      "page id =  292 2\n",
      "page id =  293 2\n",
      "page id =  294 2\n",
      "page id =  295 2\n",
      "page id =  296 2\n",
      "page id =  297 2\n",
      "page id =  298 2\n",
      "page id =  299 2\n",
      "page id =  300 2\n",
      "page id =  301 2\n",
      "page id =  302 2\n",
      "page id =  303 2\n",
      "page id =  304 2\n",
      "page id =  305 2\n",
      "page id =  306 2\n",
      "page id =  132 3\n",
      "page id =  133 3\n",
      "page id =  134 3\n",
      "page id =  135 3\n",
      "page id =  136 3\n",
      "page id =  137 3\n",
      "page id =  138 3\n",
      "page id =  139 3\n",
      "page id =  140 3\n",
      "page id =  141 3\n",
      "page id =  142 3\n",
      "page id =  143 3\n",
      "page id =  144 3\n",
      "page id =  145 3\n",
      "page id =  146 3\n",
      "page id =  147 3\n",
      "page id =  148 3\n",
      "page id =  149 3\n",
      "page id =  150 3\n",
      "page id =  151 3\n",
      "page id =  152 3\n",
      "page id =  153 3\n",
      "page id =  154 3\n",
      "page id =  155 3\n",
      "page id =  156 3\n",
      "page id =  157 3\n",
      "page id =  158 3\n",
      "page id =  159 3\n",
      "page id =  160 3\n",
      "page id =  161 3\n",
      "page id =  162 3\n",
      "page id =  163 3\n",
      "page id =  164 3\n",
      "page id =  165 3\n",
      "page id =  166 3\n",
      "page id =  167 3\n",
      "page id =  168 3\n",
      "page id =  169 3\n",
      "page id =  170 3\n",
      "page id =  171 3\n",
      "page id =  172 3\n",
      "page id =  173 3\n",
      "page id =  174 3\n",
      "page id =  175 3\n",
      "page id =  181 3\n",
      "page id =  182 3\n",
      "page id =  183 3\n",
      "page id =  184 3\n",
      "page id =  185 3\n",
      "page id =  186 3\n",
      "page id =  187 3\n",
      "page id =  188 3\n",
      "page id =  189 3\n",
      "page id =  190 3\n",
      "page id =  191 3\n",
      "page id =  192 3\n",
      "page id =  193 3\n",
      "page id =  194 3\n",
      "page id =  195 3\n",
      "page id =  196 3\n",
      "page id =  197 3\n",
      "page id =  198 3\n",
      "page id =  199 3\n",
      "page id =  200 3\n",
      "page id =  201 3\n",
      "page id =  202 3\n",
      "page id =  203 3\n",
      "page id =  204 3\n",
      "page id =  205 3\n",
      "page id =  206 3\n",
      "page id =  207 3\n",
      "page id =  208 3\n",
      "page id =  210 3\n",
      "page id =  211 3\n",
      "page id =  212 3\n",
      "page id =  213 3\n",
      "page id =  214 3\n",
      "page id =  216 3\n",
      "page id =  218 3\n",
      "page id =  219 3\n",
      "page id =  220 3\n",
      "page id =  221 3\n",
      "page id =  222 3\n",
      "page id =  223 3\n",
      "page id =  224 3\n",
      "page id =  225 3\n",
      "page id =  226 3\n",
      "page id =  228 3\n",
      "page id =  229 3\n",
      "page id =  230 3\n",
      "page id =  231 3\n",
      "page id =  232 3\n",
      "page id =  233 3\n",
      "page id =  234 3\n",
      "page id =  235 3\n",
      "page id =  236 3\n",
      "page id =  237 3\n",
      "page id =  238 3\n",
      "page id =  239 3\n",
      "page id =  240 3\n",
      "page id =  241 3\n",
      "page id =  242 3\n",
      "page id =  243 3\n",
      "page id =  244 3\n",
      "page id =  245 3\n",
      "page id =  246 3\n",
      "page id =  247 3\n",
      "page id =  248 3\n",
      "page id =  250 3\n",
      "page id =  251 3\n",
      "page id =  252 3\n",
      "page id =  253 3\n",
      "page id =  254 3\n",
      "page id =  255 3\n",
      "page id =  256 3\n",
      "page id =  257 3\n",
      "page id =  258 3\n",
      "page id =  259 3\n",
      "page id =  260 3\n",
      "page id =  261 3\n",
      "page id =  262 3\n",
      "page id =  263 3\n",
      "page id =  264 3\n",
      "page id =  265 3\n",
      "page id =  266 3\n",
      "page id =  267 3\n",
      "page id =  279 3\n",
      "page id =  281 3\n",
      "page id =  283 3\n",
      "page id =  284 3\n",
      "page id =  285 3\n",
      "page id =  286 3\n",
      "page id =  287 3\n",
      "page id =  288 3\n",
      "page id =  289 3\n",
      "page id =  290 3\n",
      "page id =  291 3\n",
      "page id =  292 3\n",
      "page id =  293 3\n",
      "page id =  294 3\n",
      "page id =  295 3\n",
      "page id =  296 3\n",
      "page id =  297 3\n",
      "page id =  298 3\n",
      "page id =  299 3\n",
      "page id =  300 3\n",
      "page id =  301 3\n",
      "page id =  302 3\n",
      "page id =  303 3\n",
      "page id =  304 3\n",
      "page id =  305 3\n",
      "page id =  306 3\n",
      "page id =  132 4\n",
      "page id =  133 4\n",
      "page id =  134 4\n",
      "page id =  135 4\n",
      "page id =  136 4\n",
      "page id =  137 4\n",
      "page id =  138 4\n",
      "page id =  139 4\n",
      "page id =  140 4\n",
      "page id =  141 4\n",
      "page id =  142 4\n",
      "page id =  143 4\n",
      "page id =  144 4\n",
      "page id =  145 4\n",
      "page id =  146 4\n",
      "page id =  147 4\n",
      "page id =  148 4\n",
      "page id =  149 4\n",
      "page id =  150 4\n",
      "page id =  151 4\n",
      "page id =  152 4\n",
      "page id =  153 4\n",
      "page id =  154 4\n",
      "page id =  155 4\n",
      "page id =  156 4\n",
      "page id =  157 4\n",
      "page id =  158 4\n",
      "page id =  159 4\n",
      "page id =  160 4\n",
      "page id =  161 4\n",
      "page id =  162 4\n",
      "page id =  163 4\n",
      "page id =  164 4\n",
      "page id =  165 4\n",
      "page id =  166 4\n",
      "page id =  167 4\n",
      "page id =  168 4\n",
      "page id =  169 4\n",
      "page id =  170 4\n",
      "page id =  171 4\n",
      "page id =  172 4\n",
      "page id =  173 4\n",
      "page id =  174 4\n",
      "page id =  175 4\n",
      "page id =  181 4\n",
      "page id =  182 4\n",
      "page id =  183 4\n",
      "page id =  184 4\n",
      "page id =  185 4\n",
      "page id =  186 4\n",
      "page id =  187 4\n",
      "page id =  188 4\n",
      "page id =  189 4\n",
      "page id =  190 4\n",
      "page id =  191 4\n",
      "page id =  192 4\n",
      "page id =  193 4\n",
      "page id =  194 4\n",
      "page id =  195 4\n",
      "page id =  196 4\n",
      "page id =  197 4\n",
      "page id =  198 4\n",
      "page id =  199 4\n",
      "page id =  200 4\n",
      "page id =  201 4\n",
      "page id =  202 4\n",
      "page id =  203 4\n",
      "page id =  204 4\n",
      "page id =  205 4\n",
      "page id =  206 4\n",
      "page id =  207 4\n",
      "page id =  208 4\n",
      "page id =  210 4\n",
      "page id =  211 4\n",
      "page id =  212 4\n",
      "page id =  213 4\n",
      "page id =  214 4\n",
      "page id =  216 4\n",
      "page id =  218 4\n",
      "page id =  219 4\n",
      "page id =  220 4\n",
      "page id =  221 4\n",
      "page id =  222 4\n",
      "page id =  223 4\n",
      "page id =  224 4\n",
      "page id =  225 4\n",
      "page id =  226 4\n",
      "page id =  228 4\n",
      "page id =  229 4\n",
      "page id =  230 4\n",
      "page id =  231 4\n",
      "page id =  232 4\n",
      "page id =  233 4\n",
      "page id =  234 4\n",
      "page id =  235 4\n",
      "page id =  236 4\n",
      "page id =  237 4\n",
      "page id =  238 4\n",
      "page id =  239 4\n",
      "page id =  240 4\n",
      "page id =  241 4\n",
      "page id =  242 4\n",
      "page id =  243 4\n",
      "page id =  244 4\n",
      "page id =  245 4\n",
      "page id =  246 4\n",
      "page id =  247 4\n",
      "page id =  248 4\n",
      "page id =  250 4\n",
      "page id =  251 4\n",
      "page id =  252 4\n",
      "page id =  253 4\n",
      "page id =  254 4\n",
      "page id =  255 4\n",
      "page id =  256 4\n",
      "page id =  257 4\n",
      "page id =  258 4\n",
      "page id =  259 4\n",
      "page id =  260 4\n",
      "page id =  261 4\n",
      "page id =  262 4\n",
      "page id =  263 4\n",
      "page id =  264 4\n",
      "page id =  265 4\n",
      "page id =  266 4\n",
      "page id =  267 4\n",
      "page id =  279 4\n",
      "page id =  281 4\n",
      "page id =  283 4\n",
      "page id =  284 4\n",
      "page id =  285 4\n",
      "page id =  286 4\n",
      "page id =  287 4\n",
      "page id =  288 4\n",
      "page id =  289 4\n",
      "page id =  290 4\n",
      "page id =  291 4\n",
      "page id =  292 4\n",
      "page id =  293 4\n",
      "page id =  294 4\n",
      "page id =  295 4\n",
      "page id =  296 4\n",
      "page id =  297 4\n",
      "page id =  298 4\n",
      "page id =  299 4\n",
      "page id =  300 4\n",
      "page id =  301 4\n",
      "page id =  302 4\n",
      "page id =  303 4\n",
      "page id =  304 4\n",
      "page id =  305 4\n",
      "page id =  306 4\n",
      "page id =  132 5\n",
      "page id =  133 5\n",
      "page id =  134 5\n",
      "page id =  135 5\n",
      "page id =  136 5\n",
      "page id =  137 5\n",
      "page id =  138 5\n",
      "page id =  139 5\n",
      "page id =  140 5\n",
      "page id =  141 5\n",
      "page id =  142 5\n",
      "page id =  143 5\n",
      "page id =  144 5\n",
      "page id =  145 5\n",
      "page id =  146 5\n",
      "page id =  147 5\n",
      "page id =  148 5\n",
      "page id =  149 5\n",
      "page id =  150 5\n",
      "page id =  151 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page id =  152 5\n",
      "page id =  153 5\n",
      "page id =  154 5\n",
      "page id =  155 5\n",
      "page id =  156 5\n",
      "page id =  157 5\n",
      "page id =  158 5\n",
      "page id =  159 5\n",
      "page id =  160 5\n",
      "page id =  161 5\n",
      "page id =  162 5\n",
      "page id =  163 5\n",
      "page id =  164 5\n",
      "page id =  165 5\n",
      "page id =  166 5\n",
      "page id =  167 5\n",
      "page id =  168 5\n",
      "page id =  169 5\n",
      "page id =  170 5\n",
      "page id =  171 5\n",
      "page id =  172 5\n",
      "page id =  173 5\n",
      "page id =  174 5\n",
      "page id =  175 5\n",
      "page id =  181 5\n",
      "page id =  182 5\n",
      "page id =  183 5\n",
      "page id =  184 5\n",
      "page id =  185 5\n",
      "page id =  186 5\n",
      "page id =  187 5\n",
      "page id =  188 5\n",
      "page id =  189 5\n",
      "page id =  190 5\n",
      "page id =  191 5\n",
      "page id =  192 5\n",
      "page id =  193 5\n",
      "page id =  194 5\n",
      "page id =  195 5\n",
      "page id =  196 5\n",
      "page id =  197 5\n",
      "page id =  198 5\n",
      "page id =  199 5\n",
      "page id =  200 5\n",
      "page id =  201 5\n",
      "page id =  202 5\n",
      "page id =  203 5\n",
      "page id =  204 5\n",
      "page id =  205 5\n",
      "page id =  206 5\n",
      "page id =  207 5\n",
      "page id =  208 5\n",
      "page id =  210 5\n",
      "page id =  211 5\n",
      "page id =  212 5\n",
      "page id =  213 5\n",
      "page id =  214 5\n",
      "page id =  216 5\n",
      "page id =  218 5\n",
      "page id =  219 5\n",
      "page id =  220 5\n",
      "page id =  221 5\n",
      "page id =  222 5\n",
      "page id =  223 5\n",
      "page id =  224 5\n",
      "page id =  225 5\n",
      "page id =  226 5\n",
      "page id =  228 5\n",
      "page id =  229 5\n",
      "page id =  230 5\n",
      "page id =  231 5\n",
      "page id =  232 5\n",
      "page id =  233 5\n",
      "page id =  234 5\n",
      "page id =  235 5\n",
      "page id =  236 5\n",
      "page id =  237 5\n",
      "page id =  238 5\n",
      "page id =  239 5\n",
      "page id =  240 5\n",
      "page id =  241 5\n",
      "page id =  242 5\n",
      "page id =  243 5\n",
      "page id =  244 5\n",
      "page id =  245 5\n",
      "page id =  246 5\n",
      "page id =  247 5\n",
      "page id =  248 5\n",
      "page id =  250 5\n",
      "page id =  251 5\n",
      "page id =  252 5\n",
      "page id =  253 5\n",
      "page id =  254 5\n",
      "page id =  255 5\n",
      "page id =  256 5\n",
      "page id =  257 5\n",
      "page id =  258 5\n",
      "page id =  259 5\n",
      "page id =  260 5\n",
      "page id =  261 5\n",
      "page id =  262 5\n",
      "page id =  263 5\n",
      "page id =  264 5\n",
      "page id =  265 5\n",
      "page id =  266 5\n",
      "page id =  267 5\n",
      "page id =  279 5\n",
      "page id =  281 5\n",
      "page id =  283 5\n",
      "page id =  284 5\n",
      "page id =  285 5\n",
      "page id =  286 5\n",
      "page id =  287 5\n",
      "page id =  288 5\n",
      "page id =  289 5\n",
      "page id =  290 5\n",
      "page id =  291 5\n",
      "page id =  292 5\n",
      "page id =  293 5\n",
      "page id =  294 5\n",
      "page id =  295 5\n",
      "page id =  296 5\n",
      "page id =  297 5\n",
      "page id =  298 5\n",
      "page id =  299 5\n",
      "page id =  300 5\n",
      "page id =  301 5\n",
      "page id =  302 5\n",
      "page id =  303 5\n",
      "page id =  304 5\n",
      "page id =  305 5\n",
      "page id =  306 5\n"
     ]
    }
   ],
   "source": [
    "# LIME parameters\n",
    "num_features = 10\n",
    "\n",
    "# experiment parameters\n",
    "debug_display = False\n",
    "MIN_TXT_SIZE = 500\n",
    "rand_control_num = 500           # number of times to bootstrap random samples from text\n",
    "num_samples = 5000                # number of samples LIME uses to build its model\n",
    "stop_words = False                # True - stop words left in ; False - stop words left out \n",
    "\n",
    "num_articles = len(page_id_list)\n",
    "\n",
    "# for speed cache stop words\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def open_results_files(ts):\n",
    "  ts = time.localtime()\n",
    "  results_file = \"results/results_\" + time.strftime(\"grid_%d%m%Y_%H%M%S.json\", ts)\n",
    "  errors_file = \"results/errors_\" + time.strftime(\"grid_%d%m%Y_%H%M%S.txt\", ts)\n",
    "  try:\n",
    "    fr = open(results_file,'w')\n",
    "  except:\n",
    "    print(\"error opening file %s\" % (results_file))\n",
    "  try:\n",
    "    fe = open(errors_file,'w')\n",
    "  except:\n",
    "    print(\"error opening file %s\" % (errors_file)) \n",
    "  return fr, fe \n",
    "\n",
    "###################################################\n",
    "## CREATE EXPLAINER OBJECT                       ##\n",
    "###################################################\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(split_expression=r'\\W+',class_names=class_names,ngram_size=1,stop_words=stop_words)\n",
    "\n",
    "# def run_experiment(num_articles, num_features,num_samples)\n",
    "# ********** MAIN LOOP *************\n",
    "# look at first [num_articles] pages\n",
    "all_word_scores = []\n",
    "all_random_word_scores = []\n",
    "\n",
    "ts = time.localtime()\n",
    "timestamp =  time.strftime(\"%H%M%S_%d%m%Y\", ts)\n",
    "\n",
    "desc_string = \"500 sets of random words, added word weight from LIME\"\n",
    "short_desc = \"E200 current master version - unigrams, stop words removed; v2 of data\"\n",
    "# setup trials for grid search\n",
    "trials = {1:500,2:1000,3:3000,4:4000,5:5000}\n",
    "\n",
    "\n",
    "#  note - JSON keys have to be strings (unlike python dict keys)\n",
    "experiment_dict = {'timestamp':timestamp,\n",
    "                   'description': desc_string ,\n",
    "                   'short_desc' : short_desc,\n",
    "                     'trials':\n",
    "                       {str(trial):\n",
    "                         {\n",
    "                           'num_articles':num_articles,\n",
    "                           'num_features':num_features,\n",
    "                           'num_samples':num_samples,\n",
    "                           'rand_sample_size':rand_control_num,\n",
    "                           'stop_words': stop_words,\n",
    "                           'corpus':test_file,\n",
    "                           'run_time':'',\n",
    "                           'articles':\n",
    "                             {str(page_id):\n",
    "                               {\n",
    "                               'score':0.0,\n",
    "                               'txt_len':0,\n",
    "                               'quote_len':0,\n",
    "                               'explainer_words':[],\n",
    "                               'random_words':[]\n",
    "                               } for page_id in page_id_list[:num_articles]                    \n",
    "                             }\n",
    "                         }for trial in trials\n",
    "                       }\n",
    "                  }\n",
    "                    \n",
    "\n",
    "res_file, err_file = open_results_files(ts)\n",
    "\n",
    "\n",
    "page_dict = {}\n",
    "# how many times to select random word comparision set from same text \n",
    "\n",
    "trial_key = str(0)\n",
    "# for page_id in page_id_list[:num_articles]:\n",
    "# for page_id in page_id_list:\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "## LOOP THROUGH ARTICLES                         ##\n",
    "###################################################\n",
    "for trial in trials:\n",
    "  trial_key = str(trial)\n",
    "  experiment_dict['trials'][trial_key]['num_samples'] = trials[trial]\n",
    "  num_samples = trials[trial]\n",
    "  for page_id in page_id_list:\n",
    "#   for page_id in page_id_list[:2]:\n",
    "    page_key = str(page_id)\n",
    "    print(\"page id = \",page_id,trial)\n",
    "  #  get text of article\n",
    "    text = testing[testing['page__id'] == page_id].iloc[0]['text']\n",
    "    text = text.lower()\n",
    "\n",
    "  #   skip to next article, if article text is too small\n",
    "    if len(text) < MIN_TXT_SIZE:\n",
    "      continue\n",
    "\n",
    "    #   Run LIME explainer to get explainer words\n",
    "    exp = explainer.explain_instance(text, pred_proba_func, num_features=num_features, num_samples=num_samples)\n",
    "    if debug_display:\n",
    "      print('Probability(biased) =', c.predict_proba([text])[0,0])\n",
    "    experiment_dict['trials'][trial_key]['articles'][page_key]['score'] = exp.score\n",
    "    exp_words = [word for (word,weight) in exp.as_list()]\n",
    "\n",
    "    # find all quotes for given article\n",
    "    quotes_list_df = get_quotes_list(testing, page_id)\n",
    "    quotes_list = [quote.lower() for quote in quotes_list_df['quote']]\n",
    "    quotes_idxs = get_quote_idxs(text, quotes_list, err_file)\n",
    "    quote_regions = build_quoted_regions(quotes_idxs)\n",
    "\n",
    "    text_as_list = np.array(re.split(r'%s|$' % r'\\W+' , text))\n",
    "    text_count_dict = Counter(text_as_list) \n",
    "\n",
    "  #   process text into a set of words, removing single character words and stop words (if required)\n",
    "    text_as_list = np.array(list(filter(lambda x: len(x)>1, text_as_list)))\n",
    "    text_as_set = set(text_as_list)\n",
    "    if stop_words == False:\n",
    "      text_as_set = set(filter(lambda x: x not in cachedStopWords,text_as_set))  \n",
    "\n",
    "    text_as_set_list  = np.array(list(text_as_set))\n",
    "\n",
    "    word_scores = get_word_scores(exp.as_list(), text, text_count_dict, quote_regions )\n",
    "  #   all_word_scores.append(word_scores)\n",
    "    experiment_dict['trials'][trial_key]['articles'][page_key]['explainer_words'] = word_scores\n",
    "\n",
    "    #   control group of words randomly selected from article    \n",
    "    for i in range(rand_control_num):\n",
    "      all_random_word_scores = []\n",
    "      rand_choice = np.random.choice(text_as_set_list.size, num_features, replace=False)\n",
    "      random_words = [(random,0.0) for random in text_as_set_list[rand_choice]]    \n",
    "      if debug_display:\n",
    "        print(random_words)\n",
    "      random_word_scores = get_word_scores(random_words,text,text_count_dict, quote_regions)\n",
    "      experiment_dict['trials'][trial_key]['articles'][page_key]['random_words'].append(random_word_scores)\n",
    "\n",
    "\n",
    "json.dump(experiment_dict,res_file)\n",
    "res_file.close()\n",
    "err_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"he\" in cachedStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
