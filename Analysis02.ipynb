{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    results_225425_26072018 - unigram, 20 * bootstrap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analysis using \"raw counts\" - i.e. number of times a word occurs in annotated sections (counting each\n",
    "occurence separately ) \n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "latest_file = False \n",
    "\n",
    "# data_path = \"/Users/rick/factmata/factnlp-experimental/hyperpartisanship/datasets/\"\n",
    "data_path = \"/Users/rick/factmata/factnlp-experimental/explainability/results/\"\n",
    "\n",
    "# filename = \"results_224342_02082018.json\"\n",
    "# filename = \"results_164156_18082018.json\"\n",
    "filename = \"results_143523_12082018.json\"   # bigrams\n",
    "\n",
    "if latest_file == True :\n",
    "  results_files = data_path + \"results*\"\n",
    "  list_of_files = glob.glob(results_files)\n",
    "  latest_file = max(list_of_files, key=os.path.getctime)\n",
    "  filepath = latest_file\n",
    "else:\n",
    "  filepath = data_path + filename\n",
    " \n",
    "f = open(filepath,\"r\")\n",
    "experiment_dict = json.load(f)\n",
    "f.close()\n",
    "\n",
    "summary_dict = defaultdict(lambda:{'exp_count':0,'random_count':0})\n",
    "\n",
    "for trial in experiment_dict['trials']:\n",
    "    current_trial = experiment_dict['trials'][trial]['articles']\n",
    "    debug_count = 0\n",
    "    raw_exp_counts = []\n",
    "    raw_rand_counts = []\n",
    "    article_count = 0 \n",
    "    for article in current_trial:\n",
    "        article_count += 1\n",
    "        # get annontation counts for explainer words\n",
    "        word_list = current_trial[article]['explainer_words']\n",
    "        hits =[word_list[word]['annt_count'] for word in word_list]\n",
    "        exp_total = np.sum(hits)\n",
    "        summary_dict[exp_total]['exp_count'] += 1\n",
    "        raw_exp_counts.append(exp_total)\n",
    "        \n",
    "        # get annontation counts for random words\n",
    "        random_trials = current_trial[article]['random_words']\n",
    "        rand_total = 0.0\n",
    "        rand_count = 0 \n",
    "        for rt in random_trials:\n",
    "            rand_count += 1\n",
    "            hits = [rt[word]['annt_count'] for word in rt]\n",
    "            rand_total = np.sum(hits)\n",
    "            summary_dict[rand_total]['random_count'] += 1\n",
    "            raw_rand_counts.append(rand_total)\n",
    "#         print('article_count, article, rand_count = ',article_count,article, rand_count)\n",
    "    list_len = max(summary_dict.keys()) + 1 \n",
    "    \n",
    "    lime_words_total = np.sum([summary_dict[i]['exp_count'] for i in summary_dict])\n",
    "    random_words_total = np.sum([summary_dict[i]['random_count'] for i in summary_dict])\n",
    "    lime_words_percent = np.zeros(list_len,dtype=float)\n",
    "    random_words_percent = np.zeros(list_len,dtype=float) \n",
    "    for i in range(list_len):\n",
    "        if i in summary_dict:\n",
    "            lime_words_percent[i] = summary_dict[i]['exp_count'] * 100/ lime_words_total\n",
    "            random_words_percent[i] = summary_dict[i]['random_count'] * 100 / random_words_total\n",
    "#             print(\"%.4d %4d %4d\" % (i, summary_dict[i]['exp_count'],summary_dict[i]['random_count']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def t_test_and_summary(a,b,a_name=\"sample a\",b_name=\"sample_b\"):\n",
    "  t_stat,pvalue = scipy.stats.ttest_ind(a,b)\n",
    "  print(\"%-20s mean = %.2f   sd = %.2f\"%(a_name,np.mean(a),np.std(a)))\n",
    "  print(\"%-20s mean = %.2f   sd = %.2f\"%(b_name,np.mean(b),np.std(b)))\n",
    "  print(\"t statistic = %.4f\"%(t_stat))\n",
    "  print(\"p value     = %.4f\"%(pvalue))\n",
    "  \n",
    "# odds = []\n",
    "# evens = []\n",
    "# for i in range(len(raw_exp_counts) - 1):\n",
    "#   evens.append(raw_exp_counts[i])\n",
    "#   odds.append(raw_exp_counts[i+1])\n",
    "\n",
    "# t_test_and_summary(odds,evens,a_name=\"LIME words\",b_name = \"random words\")  \n",
    "t_test_and_summary(raw_exp_counts,raw_rand_counts,a_name=\"LIME words\",b_name = \"random words\")  \n",
    "\n",
    "\n",
    "# t_stat,pvalue = scipy.stats.ttest_ind(raw_exp_counts,raw_rand_counts)\n",
    "# t_stat,pvalue = scipy.stats.ttest_ind(raw_exp_counts[:75],raw_exp_counts[75:])\n",
    "# print(np.mean(raw_exp_counts),np.mean(raw_rand_counts))\n",
    "# print(np.std(raw_exp_counts),np.std(raw_rand_counts))\n",
    "# print(t_stat,pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in experiment_dict['trials']:\n",
    "    current_trial = experiment_dict['trials'][trial]['articles']\n",
    "    debug_count = 0\n",
    "    \n",
    "    expl_counts = []\n",
    "    rand_counts = []\n",
    "    for article in current_trial:\n",
    "        # get annontation counts for explainer words\n",
    "        word_list = current_trial[article]['explainer_words']\n",
    "        hits =[word_list[word]['annt_count'] for word in word_list]\n",
    "        expl_counts.append(np.sum(hits))\n",
    "        \n",
    "        \n",
    "        # get annontation counts for random words\n",
    "        random_trials = current_trial[article]['random_words']\n",
    "        rand_total = 0.0\n",
    "        for rt in random_trials:\n",
    "            hits = [rt[word]['annt_count'] for word in rt]\n",
    "            rand_counts.append(np.sum(hits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime_words_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_words_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "n_groups = len(lime_words_percent)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = ax.bar(index, lime_words_percent, bar_width,\n",
    "                alpha=opacity, color='b',\n",
    "                 error_kw=error_config,\n",
    "                label='LIME words')\n",
    "\n",
    "rects2 = ax.bar(index + bar_width, random_words_percent, bar_width,\n",
    "                alpha=opacity, color='r',\n",
    "                error_kw=error_config,\n",
    "                label='Random words')\n",
    "\n",
    "ax.set_xlabel('Occurences in annotation')\n",
    "ax.set_ylabel('Percent')\n",
    "ax.set_title('In annotation occurences by word type')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "# ax.set_xticklabels(('A', 'B', 'C', 'D', 'E'))\n",
    "ax.set_xticklabels([str(i) for i in range(n_groups)])\n",
    "\n",
    "ax.legend()\n",
    "  \n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "t_test_and_summary(raw_exp_counts,raw_rand_counts,a_name=\"LIME words\",b_name = \"random words\") \n",
    "print(\"\\n\")\n",
    "print(\"notes : \",experiment_dict['description'])\n",
    "if 'short_desc' in experiment_dict.keys():\n",
    "  print(\"notes: \",experiment_dict['short_desc'])\n",
    "  \n",
    "print(\"results file : \",filepath)\n",
    "print(\"LIME data cloud size = %d  No of LIME features = %d \" %  \n",
    "      (experiment_dict['trials']['0']['num_samples'],experiment_dict['trials']['0']['num_features']))\n",
    "if 'rand_sample_size' in experiment_dict['trials']['0'].keys():\n",
    "        print(\"random sample size = \",experiment_dict['trials']['0']['rand_sample_size'])\n",
    "if 'stop_words' in experiment_dict['trials']['0'].keys():\n",
    "        print(\"stop_words = \",experiment_dict['trials']['0']['stop_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_dict['trials']['0'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_dict['trials']['0']['articles']['132'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_dict['trials']['0']['articles']['132']['explainer_words'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_dict['trials']['0']['articles']['132']['explainer_words']['obama'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
